{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.chdir('..')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.llama import LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = \"/home/jovyan/kuratov/models/Llama-3.2-1B-Instruct/\"\n",
    "# llm = LLM(model=model, gpu_memory_utilization=0.1, max_seq_len_to_capture=1024, max_logprobs=128000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = llm.generate(\"Who are you?\")\n",
    "\n",
    "# for output in outputs:\n",
    "#     prompt = output.prompt\n",
    "#     generated_text = output.outputs[0].text\n",
    "#     print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_outputs(outputs):\n",
    "        for output in outputs:\n",
    "            prompt = output.prompt\n",
    "            generated_text = output.outputs[0].text\n",
    "            print(f\"Prompt: {prompt!r}\")\n",
    "            print(f\"Generated text: {generated_text!r}\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"/home/jovyan/kuratov/models/Llama-3.2-1B-Instruct/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-21 14:12:38 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 02-21 14:12:54 config.py:549] This model supports multiple tasks: {'embed', 'reward', 'classify', 'score', 'generate'}. Defaulting to 'generate'.\n",
      "WARNING 02-21 14:12:54 arg_utils.py:1187] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 02-21 14:12:54 config.py:1555] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 02-21 14:12:54 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/home/jovyan/kuratov/models/Llama-3.2-1B-Instruct/', speculative_config=None, tokenizer='/home/jovyan/kuratov/models/Llama-3.2-1B-Instruct/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/jovyan/kuratov/models/Llama-3.2-1B-Instruct/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 02-21 14:12:55 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 02-21 14:12:56 model_runner.py:1110] Starting to load model /home/jovyan/kuratov/models/Llama-3.2-1B-Instruct/...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22572ce7b32343f8b66391bd455a1b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-21 14:12:57 model_runner.py:1115] Loading model weights took 2.3185 GB\n",
      "INFO 02-21 14:12:57 worker.py:267] Memory profiling takes 0.46 seconds\n",
      "INFO 02-21 14:12:57 worker.py:267] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.95) = 75.29GiB\n",
      "INFO 02-21 14:12:57 worker.py:267] model weights take 2.32GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 71.70GiB.\n",
      "INFO 02-21 14:12:58 executor_base.py:111] # cuda blocks: 146843, # CPU blocks: 8192\n",
      "INFO 02-21 14:12:58 executor_base.py:116] Maximum concurrency for 131072 tokens per request: 17.93x\n",
      "INFO 02-21 14:13:00 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:12<00:00,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-21 14:13:13 model_runner.py:1562] Graph capturing finished in 13 secs, took 0.14 GiB\n",
      "INFO 02-21 14:13:13 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 16.44 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# llm = LLM(model=model, gpu_memory_utilization=0.95, max_seq_len_to_capture=128, task='classify')#max_logprobs=128000)\n",
    "llm = LLM(model=model, gpu_memory_utilization=0.95, max_seq_len_to_capture=128, max_logprobs=128000)\n",
    "# sampling_params = SamplingParams(max_tokens=10,)\n",
    "# outputs = llm.chat(conversation, sampling_params, use_tqdm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 21 Feb 2025\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nAnalyze the impact of renewable energy on climate change mitigation strategies.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'\n",
      "Generated text: \"Renewable energy has emerged as a crucial component in the fight against climate change. The transition to renewable energy can significantly reduce greenhouse gas emissions and help mitigate its impacts on the environment. Here's an analysis of the impact of renewable energy on climate change mitigation strategies:\\n\\n**Benefits of Renewable Energy in Climate Change Mitigation:**\\n\\n1. **Reducing Greenhouse Gas Emissions**: Renewable energy sources like solar, wind, and hydroelectric power emit fewer greenhouse gases (GHGs) than fossil fuels.\"\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\":\n",
    "            \"Analyze the impact of renewable energy on climate change mitigation strategies.\"\n",
    "        },\n",
    "    ]\n",
    "sampling_params = SamplingParams(max_tokens=100, prompt_logprobs=True)\n",
    "outputs = llm.chat(conversation, sampling_params, use_tqdm=False)\n",
    "print_outputs(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " {128000: Logprob(logprob=-11.568145751953125, rank=25582, decoded_token=''),\n",
       "  16309: Logprob(logprob=-5.443145751953125, rank=1, decoded_token='Tags')},\n",
       " {128006: Logprob(logprob=-14.177520751953125, rank=124697, decoded_token='<|start_header_id|>'),\n",
       "  16309: Logprob(logprob=-5.443145751953125, rank=1, decoded_token='Tags')},\n",
       " {9125: Logprob(logprob=-24.096464157104492, rank=123832, decoded_token='system'),\n",
       "  128006: Logprob(logprob=-0.002713571535423398, rank=1, decoded_token='<|start_header_id|>')},\n",
       " {128007: Logprob(logprob=-10.984933853149414, rank=5314, decoded_token='<|end_header_id|>'),\n",
       "  198: Logprob(logprob=-1.9458709955215454, rank=1, decoded_token='\\n')},\n",
       " {271: Logprob(logprob=-3.08882474899292, rank=2, decoded_token='\\n\\n'),\n",
       "  78191: Logprob(logprob=-0.08882471174001694, rank=1, decoded_token='assistant')},\n",
       " {38766: Logprob(logprob=-14.02652645111084, rank=1965, decoded_token='Cut'),\n",
       "  567: Logprob(logprob=-0.08902616053819656, rank=1, decoded_token='##')},\n",
       " {1303: Logprob(logprob=-1.0875838994979858, rank=1, decoded_token='ting')},\n",
       " {33025: Logprob(logprob=-6.706644535064697, rank=15, decoded_token=' Knowledge'),\n",
       "  311: Logprob(logprob=-0.706644594669342, rank=1, decoded_token=' to')},\n",
       " {2696: Logprob(logprob=-3.7245562076568604, rank=6, decoded_token=' Date'),\n",
       "  12299: Logprob(logprob=-2.5995562076568604, rank=1, decoded_token=' Area')},\n",
       " {25: Logprob(logprob=-0.0008893824997358024, rank=1, decoded_token=':')},\n",
       " {6790: Logprob(logprob=-8.254685401916504, rank=4, decoded_token=' December'),\n",
       "  220: Logprob(logprob=-0.004685494117438793, rank=1, decoded_token=' ')},\n",
       " {220: Logprob(logprob=-4.768370445162873e-07, rank=1, decoded_token=' ')},\n",
       " {2366: Logprob(logprob=-1.9643769264221191, rank=2, decoded_token='202'),\n",
       "  16: Logprob(logprob=-0.33937686681747437, rank=1, decoded_token='1')},\n",
       " {18: Logprob(logprob=-5.305409908294678, rank=4, decoded_token='3'),\n",
       "  15: Logprob(logprob=-0.1804097443819046, rank=1, decoded_token='0')},\n",
       " {198: Logprob(logprob=-0.2432992309331894, rank=1, decoded_token='\\n')},\n",
       " {15724: Logprob(logprob=-8.032332420349121, rank=280, decoded_token='Today'),\n",
       "  9673: Logprob(logprob=-2.469832181930542, rank=1, decoded_token='These')},\n",
       " {2696: Logprob(logprob=-6.808374881744385, rank=5, decoded_token=' Date'),\n",
       "  596: Logprob(logprob=-0.05837501212954521, rank=1, decoded_token=\"'s\")},\n",
       " {25: Logprob(logprob=-0.0004015354788862169, rank=1, decoded_token=':')},\n",
       " {220: Logprob(logprob=-4.22143030166626, rank=7, decoded_token=' '),\n",
       "  510: Logprob(logprob=-0.5964303016662598, rank=1, decoded_token=' [')},\n",
       " {1691: Logprob(logprob=-7.615567684173584, rank=91, decoded_token='21'),\n",
       "  2360: Logprob(logprob=-0.9280675649642944, rank=1, decoded_token=' No')},\n",
       " {13806: Logprob(logprob=-7.329261302947998, rank=17, decoded_token=' Feb'),\n",
       "  5587: Logprob(logprob=-0.954261302947998, rank=1, decoded_token=' March')},\n",
       " {220: Logprob(logprob=-0.17675335705280304, rank=1, decoded_token=' ')},\n",
       " {2366: Logprob(logprob=-6.16293036728166e-05, rank=1, decoded_token='202')},\n",
       " {20: Logprob(logprob=-13.096306800842285, rank=5, decoded_token='5'),\n",
       "  18: Logprob(logprob=-0.03380666673183441, rank=1, decoded_token='3')},\n",
       " {271: Logprob(logprob=-6.337531089782715, rank=4, decoded_token='\\n\\n'),\n",
       "  198: Logprob(logprob=-0.025031203404068947, rank=1, decoded_token='\\n')},\n",
       " {2675: Logprob(logprob=-9.395023345947266, rank=701, decoded_token='You'),\n",
       "  334: Logprob(logprob=-2.0200233459472656, rank=1, decoded_token='**')},\n",
       " {527: Logprob(logprob=-0.36322224140167236, rank=1, decoded_token=' are')},\n",
       " {264: Logprob(logprob=-1.5820378065109253, rank=1, decoded_token=' a')},\n",
       " {11190: Logprob(logprob=-8.568842887878418, rank=636, decoded_token=' helpful'),\n",
       "  5944: Logprob(logprob=-3.443843126296997, rank=1, decoded_token=' travel')},\n",
       " {18328: Logprob(logprob=-1.0212395191192627, rank=1, decoded_token=' assistant')},\n",
       " {128009: Logprob(logprob=-11.935185432434082, rank=262, decoded_token=''),\n",
       "  382: Logprob(logprob=-1.3414356708526611, rank=1, decoded_token='.\\n\\n')},\n",
       " {128006: Logprob(logprob=-0.0002474478678777814, rank=1, decoded_token='<|start_header_id|>')},\n",
       " {882: Logprob(logprob=-23.875001907348633, rank=104, decoded_token='user'),\n",
       "  78191: Logprob(logprob=-1.4305104514278355e-06, rank=1, decoded_token='assistant')},\n",
       " {128007: Logprob(logprob=-13.397916793823242, rank=30049, decoded_token='<|end_header_id|>'),\n",
       "  271: Logprob(logprob=-1.4604170322418213, rank=1, decoded_token='\\n\\n')},\n",
       " {271: Logprob(logprob=-1.1920928244535389e-07, rank=1, decoded_token='\\n\\n')},\n",
       " {2127: Logprob(logprob=-7.267465591430664, rank=68, decoded_token='An'),\n",
       "  40: Logprob(logprob=-1.2049657106399536, rank=1, decoded_token='I')},\n",
       " {56956: Logprob(logprob=-3.6090056896209717, rank=3, decoded_token='alyze'),\n",
       "  220: Logprob(logprob=-2.1715056896209717, rank=1, decoded_token=' ')},\n",
       " {279: Logprob(logprob=-0.09340374916791916, rank=1, decoded_token=' the')},\n",
       " {5536: Logprob(logprob=-2.3927085399627686, rank=1, decoded_token=' impact')},\n",
       " {315: Logprob(logprob=-0.009587906301021576, rank=1, decoded_token=' of')},\n",
       " {33268: Logprob(logprob=-7.078042030334473, rank=98, decoded_token=' renewable'),\n",
       "  279: Logprob(logprob=-2.0155420303344727, rank=1, decoded_token=' the')},\n",
       " {4907: Logprob(logprob=-0.005340831819921732, rank=1, decoded_token=' energy')},\n",
       " {389: Logprob(logprob=-4.664935111999512, rank=6, decoded_token=' on'),\n",
       "  8336: Logprob(logprob=-0.2899352014064789, rank=1, decoded_token=' sources')},\n",
       " {10182: Logprob(logprob=-4.228795051574707, rank=7, decoded_token=' climate'),\n",
       "  279: Logprob(logprob=-0.3537950813770294, rank=1, decoded_token=' the')},\n",
       " {2349: Logprob(logprob=-0.006720440462231636, rank=1, decoded_token=' change')},\n",
       " {66860: Logprob(logprob=-0.2798386812210083, rank=1, decoded_token=' mitigation')},\n",
       " {15174: Logprob(logprob=-2.1655194759368896, rank=2, decoded_token=' strategies'),\n",
       "  323: Logprob(logprob=-0.7905195355415344, rank=1, decoded_token=' and')},\n",
       " {13: Logprob(logprob=-3.174872875213623, rank=6, decoded_token='.'),\n",
       "  271: Logprob(logprob=-1.174872875213623, rank=1, decoded_token='\\n\\n')},\n",
       " {128009: Logprob(logprob=-2.9110329151153564, rank=2, decoded_token=''),\n",
       "  93438: Logprob(logprob=-0.28603288531303406, rank=1, decoded_token=' Renewable')},\n",
       " {128006: Logprob(logprob=-1.0013530300057027e-05, rank=1, decoded_token='<|start_header_id|>')},\n",
       " {78191: Logprob(logprob=-1.1920922133867862e-06, rank=1, decoded_token='assistant')},\n",
       " {128007: Logprob(logprob=-4.637133679352701e-05, rank=1, decoded_token='<|end_header_id|>')},\n",
       " {271: Logprob(logprob=0.0, rank=1, decoded_token='\\n\\n')}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].prompt_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 123.08it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.classify(\"Analyze the impact of renewable energy on climate change mitigation strategies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassificationRequestOutput(request_id='3', outputs=ClassificationOutput(num_classes=2), prompt_token_ids=[128000, 2127, 56956, 279, 5536, 315, 33268, 4907, 389, 10182, 2349, 66860, 15174, 13], finished=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs[0].prompt_logprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs[0].prompt_logprobs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(output.outputs[0].logprobs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'token'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token_info \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlogprobs:\n\u001b[0;32m---> 16\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[43mtoken_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     17\u001b[0m         logprob \u001b[38;5;241m=\u001b[39m token_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprob\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     18\u001b[0m         top_logprobs \u001b[38;5;241m=\u001b[39m token_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Dictionary of top tokens and their logprobs\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'token'"
     ]
    }
   ],
   "source": [
    "# Define your conversation\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke.\"}\n",
    "]\n",
    "\n",
    "# Set sampling parameters with logprobs to obtain logits\n",
    "sampling_params = SamplingParams(max_tokens=10, logprobs=5)\n",
    "\n",
    "# Generate outputs using the chat method\n",
    "outputs = llm.chat(conversation, sampling_params=sampling_params, use_tqdm=False)\n",
    "\n",
    "# Extract and process logits from the outputs\n",
    "for output in outputs:\n",
    "    for token_info in output.outputs[0].logprobs:\n",
    "        token = token_info[\"token\"]\n",
    "        logprob = token_info[\"logprob\"]\n",
    "        top_logprobs = token_info[\"top_logprobs\"]  # Dictionary of top tokens and their logprobs\n",
    "        print(f\"Token: {token}, Log Probability: {logprob}\")\n",
    "        print(\"Top Log Probabilities:\")\n",
    "        for top_token, top_logprob in top_logprobs.items():\n",
    "            print(f\"  {top_token}: {top_logprob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{32: Logprob(logprob=-0.17965351045131683, rank=1, decoded_token='A'),\n",
       "  8586: Logprob(logprob=-1.929653525352478, rank=2, decoded_token='Here'),\n",
       "  40: Logprob(logprob=-4.304653644561768, rank=3, decoded_token='I'),\n",
       "  10445: Logprob(logprob=-5.554653644561768, rank=4, decoded_token='Why'),\n",
       "  3923: Logprob(logprob=-7.429653644561768, rank=5, decoded_token='What')},\n",
       " {893: Logprob(logprob=-0.00468395184725523, rank=1, decoded_token=' man'),\n",
       "  4333: Logprob(logprob=-7.129683971405029, rank=2, decoded_token=' friend'),\n",
       "  7564: Logprob(logprob=-8.067183494567871, rank=3, decoded_token=' guy'),\n",
       "  5333: Logprob(logprob=-8.504683494567871, rank=4, decoded_token=' woman'),\n",
       "  37500: Logprob(logprob=-8.817183494567871, rank=5, decoded_token=' farmer')},\n",
       " {15203: Logprob(logprob=-0.23272323608398438, rank=1, decoded_token=' walked'),\n",
       "  23291: Logprob(logprob=-1.6077232360839844, rank=2, decoded_token=' walks'),\n",
       "  374: Logprob(logprob=-5.982723236083984, rank=3, decoded_token=' is'),\n",
       "  11689: Logprob(logprob=-6.107723236083984, rank=4, decoded_token=' walking'),\n",
       "  4321: Logprob(logprob=-7.107723236083984, rank=5, decoded_token=' walk')},\n",
       " {1139: Logprob(logprob=-0.00015162272029556334, rank=1, decoded_token=' into'),\n",
       "  8800: Logprob(logprob=-9.250151634216309, rank=2, decoded_token=' onto'),\n",
       "  18614: Logprob(logprob=-10.750151634216309, rank=3, decoded_token='into'),\n",
       "  304: Logprob(logprob=-11.875151634216309, rank=4, decoded_token=' in'),\n",
       "  12779: Logprob(logprob=-12.312651634216309, rank=5, decoded_token=' INTO')},\n",
       " {264: Logprob(logprob=-7.819823804311454e-05, rank=1, decoded_token=' a'),\n",
       "  459: Logprob(logprob=-9.875078201293945, rank=2, decoded_token=' an'),\n",
       "  813: Logprob(logprob=-11.875078201293945, rank=3, decoded_token=' his'),\n",
       "  279: Logprob(logprob=-12.375078201293945, rank=4, decoded_token=' the'),\n",
       "  15419: Logprob(logprob=-12.437578201293945, rank=5, decoded_token=' therapy')},\n",
       " {6875: Logprob(logprob=-0.004394636023789644, rank=1, decoded_token=' library'),\n",
       "  3703: Logprob(logprob=-5.75439453125, rank=2, decoded_token=' bar'),\n",
       "  11896: Logprob(logprob=-8.12939453125, rank=3, decoded_token=' Library'),\n",
       "  18556: Logprob(logprob=-8.56689453125, rank=4, decoded_token='library'),\n",
       "  95307: Logprob(logprob=-8.69189453125, rank=5, decoded_token=' librarian')},\n",
       " {323: Logprob(logprob=-4.684815212385729e-05, rank=1, decoded_token=' and'),\n",
       "  12512: Logprob(logprob=-10.875046730041504, rank=2, decoded_token=' wearing'),\n",
       "  13: Logprob(logprob=-12.250046730041504, rank=3, decoded_token='.'),\n",
       "  3411: Logprob(logprob=-12.500046730041504, rank=4, decoded_token=' looking'),\n",
       "  11: Logprob(logprob=-12.500046730041504, rank=5, decoded_token=',')},\n",
       " {4691: Logprob(logprob=-0.0007340597221627831, rank=1, decoded_token=' asked'),\n",
       "  17501: Logprob(logprob=-8.750734329223633, rank=2, decoded_token=' asks'),\n",
       "  1071: Logprob(logprob=-8.875734329223633, rank=3, decoded_token=' said'),\n",
       "  10371: Logprob(logprob=-9.125734329223633, rank=4, decoded_token=' asking'),\n",
       "  2610: Logprob(logprob=-9.500734329223633, rank=5, decoded_token=' ask')},\n",
       " {279: Logprob(logprob=-3.40932747349143e-05, rank=1, decoded_token=' the'),\n",
       "  264: Logprob(logprob=-10.93753433227539, rank=2, decoded_token=' a'),\n",
       "  11: Logprob(logprob=-11.50003433227539, rank=3, decoded_token=','),\n",
       "  369: Logprob(logprob=-13.18753433227539, rank=4, decoded_token=' for'),\n",
       "  813: Logprob(logprob=-14.18753433227539, rank=5, decoded_token=' his')},\n",
       " {95307: Logprob(logprob=-0.002025458961725235, rank=1, decoded_token=' librarian'),\n",
       "  61459: Logprob(logprob=-6.752025604248047, rank=2, decoded_token=' libr'),\n",
       "  5806: Logprob(logprob=-8.002025604248047, rank=3, decoded_token=' Lib'),\n",
       "  6875: Logprob(logprob=-9.064525604248047, rank=4, decoded_token=' library'),\n",
       "  5687: Logprob(logprob=-9.877025604248047, rank=5, decoded_token=' staff')}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.outputs[0].logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = outputs[0].prompt_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.add_request(str(req_id),prompt,sampling_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.llm_engine.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs[0]a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(probs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from vllm import LLM, EngineArgs\n",
    "from vllm.utils import FlexibleArgumentParser\n",
    "\n",
    "\n",
    "def main(args: dict):\n",
    "    # Pop arguments not used by LLM\n",
    "    max_tokens = args.pop(\"max_tokens\")\n",
    "    temperature = args.pop(\"temperature\")\n",
    "    top_p = args.pop(\"top_p\")\n",
    "    top_k = args.pop(\"top_k\")\n",
    "    chat_template_path = args.pop(\"chat_template_path\")\n",
    "\n",
    "    # Create an LLM\n",
    "    llm = LLM(**args)\n",
    "\n",
    "    # Create sampling params object\n",
    "    sampling_params = llm.get_default_sampling_params()\n",
    "    if max_tokens is not None:\n",
    "        sampling_params.max_tokens = max_tokens\n",
    "    if temperature is not None:\n",
    "        sampling_params.temperature = temperature\n",
    "    if top_p is not None:\n",
    "        sampling_params.top_p = top_p\n",
    "    if top_k is not None:\n",
    "        sampling_params.top_k = top_k\n",
    "\n",
    "    def print_outputs(outputs):\n",
    "        for output in outputs:\n",
    "            prompt = output.prompt\n",
    "            generated_text = output.outputs[0].text\n",
    "            print(f\"Prompt: {prompt!r}\")\n",
    "            print(f\"Generated text: {generated_text!r}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # In this script, we demonstrate how to pass input to the chat method:\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Hello! How can I assist you today?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\":\n",
    "            \"Write an essay about the importance of higher education.\",\n",
    "        },\n",
    "    ]\n",
    "    outputs = llm.chat(conversation, sampling_params, use_tqdm=False)\n",
    "    print_outputs(outputs)\n",
    "\n",
    "    # You can run batch inference with llm.chat API\n",
    "    conversations = [conversation for _ in range(10)]\n",
    "\n",
    "    # We turn on tqdm progress bar to verify it's indeed running batch inference\n",
    "    outputs = llm.chat(conversations, sampling_params, use_tqdm=True)\n",
    "    print_outputs(outputs)\n",
    "\n",
    "    # A chat template can be optionally supplied.\n",
    "    # If not, the model will use its default chat template.\n",
    "    if chat_template_path is not None:\n",
    "        with open(chat_template_path) as f:\n",
    "            chat_template = f.read()\n",
    "\n",
    "        outputs = llm.chat(\n",
    "            conversations,\n",
    "            sampling_params,\n",
    "            use_tqdm=False,\n",
    "            chat_template=chat_template,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.44s/it, est. speed input: 3.47 toks/s, output: 11.11 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Who are you?', Generated text: \" I've heard of you.\\nThat Leonard Einhorn, medical director of the Institute\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(\"Who are you?\")\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['test'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = CausalLMOutputWithCrossAttentions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import datasets\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from babilong.prompts import DEFAULT_PROMPTS, DEFAULT_TEMPLATE, get_formatted_input\n",
    "from babilong.babilong_utils import compare_answers\n",
    "\n",
    "# from modeling_amt.language_modeling import *\n",
    "from modeling_amt.experimental import *\n",
    "\n",
    "from babilong.babilong_utils import MultiTaskDataset, SentenceSampler, NoiseInjectionDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0483ee5784e54ee889236b66f53bcc22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/16.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset('RMT-team/babilong', '1k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 not in [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample = ds['qa1'][0]\n",
    "tokenizer = AutoTokenizer.from_pretrained('/home/jovyan/kuratov/models/Llama-3.2-1B-Instruct/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Where is Mary? ',\n",
       " 'input': 'According to the lawyers’ conception, whatever might be the\\npowers of Parliament when it actually came together, however much the\\nKing might be bound to act by its advice, consent, and authority, the\\nParliament itself did nevertheless derive its being from the authority\\nof the King. Parliament was summoned by the King’s writ. The King\\nmight indeed be bound to issue the writs for its summons; still it was\\nfrom the King’s writ that the Parliament actually derived its being\\nand its powers. By another legal assumption, the force of the King’s\\nwrit was held to last only during the lifetime of the King who issued\\nit. It followed therefore that Parliament, summoned by the King’s\\nwrit and deriving its authority from the King’s writ, was dissolved\\n_ipso facto_ by the death of the King who summoned it. Once admit the\\nassumptions from which this reasoning starts, and the reasoning itself\\nis perfect. Let us see how\\nthis mass of legal subtlety would have looked in the eyes of a man of\\nthe eleventh century, in the eyes of a man who had borne his part in\\nthe elections of Eadward and of Harold, and who had raised his voice\\nand clashed his arms in the great Assembly which restored Godwine to\\nhis lands and honours(14). John travelled to the hallway. Mary journeyed to the bathroom. To such an one the doctrine that a national\\nAssembly could be gathered together only by the King’s writ, and the\\nconsequent doctrine that the national Assembly ceased to exist when the\\nbreath went out of the King’s body, would have seemed like the babble\\nof a madman. When was the gathering together of the national Assembly\\nmore needed, when was it called upon to exercise higher and more\\ninherent powers, than when the throne was actually vacant, and when\\nthe Assembly of the nation came together to determine who should fill\\nit? And how could the Assembly be gathered together by the King’s writ\\nwhen there was no King in the land to issue a writ? The King’s writ\\nwould be, in his eyes, a convenient way in ordinary times for fixing\\na time and place for the meetings of the Assembly, but it would be\\nnothing more. Daniel went back to the bathroom. It would be in no sense the source of the powers of the\\nAssembly, powers which he would look upon as derived from the simple\\nfact that the Assembly was itself the nation. In his eyes it was not\\nthe King who created the Assembly, but the Assembly which created the\\nKing. The doctrine that the King never dies, that the throne never can\\nbe vacant, would have seemed gibberish to one who had seen the throne\\nvacant and had borne his part in filling it. John moved to the bedroom. The doctrine that the\\nKing can do no wrong would have seemed no less gibberish to one who\\nknew that he might possibly be called on to bear his part in deposing\\na King. Three of the most famous Assemblies in English history have\\never been puzzles in the eyes of mere legal interpreters; to the man of\\nthe eleventh century they would have seemed to be perfectly legal and\\nregular, alike in their constitution and in their',\n",
       " 'target': 'bathroom'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def formatting_func_babilong_it(example, tokenizer):\n",
    "    template = \"{} {}Answer with a single word.\"\n",
    "    context = example['input']\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": template.format(context, example['question'])},\n",
    "        {\"role\": \"assistant\", \"content\": example['target']}\n",
    "    ]\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    input_ids_short = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": template.format(context, example['question'])}],\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    labels_mask = torch.zeros(len(input_ids))\n",
    "    labels_mask[len(input_ids_short) + 1:] = True\n",
    "    return {\"text\" : input_ids, \"labels_mask\": labels_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiTaskDataset(Dataset):\n",
    "#     def __init__(self, dataset_paths, max_n_facts=None, max_n_samples=None, random_seed=42):\n",
    "#         self.datasets = [TaskDataset(p, max_n_facts=max_n_facts, max_n_samples=max_n_samples) for p in dataset_paths]\n",
    "#         self.gen = np.random.default_rng(seed=random_seed)\n",
    "\n",
    "#     def __getitem__(self, ind):\n",
    "#         dataset_ind = self.gen.choice(len(self.datasets))\n",
    "#         dataset = self.datasets[dataset_ind]\n",
    "#         return dataset[ind]\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return min([len(d) / r for d, r in zip(self.datasets, self.ratios)])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng.choice([1, 2, 3], p=(0.01, 0.01, 0.98))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_b = datasets.load_from_disk(\"/home/jovyan/rmt/armt-instruct-tune/babilong_tmp\")\n",
    "ds_d = datasets.load_from_disk(\"/home/jovyan/rmt/armt-instruct-tune/dolly_tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Column to remove ['test', 'validation', 'train'] not in the dataset. Current columns in the dataset: ['input', 'question', 'target', 'text', 'labels_mask']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m column_names \u001b[38;5;241m=\u001b[39m ds_b\u001b[38;5;241m.\u001b[39mcolumn_names\n\u001b[0;32m----> 5\u001b[0m tokenized_datasets \u001b[38;5;241m=\u001b[39m \u001b[43mds_b\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenize_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRunning tokenizer on dataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/ayd_py3.11_pt2_cu11.8/lib/python3.11/site-packages/datasets/dataset_dict.py:886\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 886\u001b[0m     \u001b[43m{\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    908\u001b[0m )\n",
      "File \u001b[0;32m~/envs/ayd_py3.11_pt2_cu11.8/lib/python3.11/site-packages/datasets/dataset_dict.py:887\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    886\u001b[0m     {\n\u001b[0;32m--> 887\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    906\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    907\u001b[0m     }\n\u001b[1;32m    908\u001b[0m )\n",
      "File \u001b[0;32m~/envs/ayd_py3.11_pt2_cu11.8/lib/python3.11/site-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    558\u001b[0m }\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/ayd_py3.11_pt2_cu11.8/lib/python3.11/site-packages/datasets/arrow_dataset.py:2994\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2992\u001b[0m     missing_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(remove_columns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names)\n\u001b[1;32m   2993\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m missing_columns:\n\u001b[0;32m-> 2994\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2995\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn to remove \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(missing_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2996\u001b[0m         )\n\u001b[1;32m   2998\u001b[0m load_from_cache_file \u001b[38;5;241m=\u001b[39m load_from_cache_file \u001b[38;5;28;01mif\u001b[39;00m load_from_cache_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_caching_enabled()\n\u001b[1;32m   3000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fn_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Column to remove ['test', 'validation', 'train'] not in the dataset. Current columns in the dataset: ['input', 'question', 'target', 'text', 'labels_mask']"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "            # return tokenizer(examples[\"text\"])\n",
    "    return examples[\"text\"]\n",
    "column_names = ds_b.column_names\n",
    "tokenized_datasets = ds_b.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Is Italian closer to French or to German?',\n",
       " 'context': '',\n",
       " 'response': 'Given the latin basis, Italian has a lot more similarities to French than to German.',\n",
       " 'category': 'open_qa',\n",
       " 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nIs Italian closer to French or to German?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nGiven the latin basis, Italian has a lot more similarities to French than to German.<|eot_id|>'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_d['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Sandra went to the bathroom. Daniel went to the bedroom. Mary went to the garden. Sandra moved to the hallway.',\n",
       " 'question': 'Where is Sandra? ',\n",
       " 'target': 'hallway',\n",
       " 'text': [128000,\n",
       "  128006,\n",
       "  9125,\n",
       "  128007,\n",
       "  271,\n",
       "  38766,\n",
       "  1303,\n",
       "  33025,\n",
       "  2696,\n",
       "  25,\n",
       "  6790,\n",
       "  220,\n",
       "  2366,\n",
       "  18,\n",
       "  198,\n",
       "  15724,\n",
       "  2696,\n",
       "  25,\n",
       "  220,\n",
       "  1627,\n",
       "  10263,\n",
       "  220,\n",
       "  2366,\n",
       "  19,\n",
       "  271,\n",
       "  128009,\n",
       "  128006,\n",
       "  882,\n",
       "  128007,\n",
       "  271,\n",
       "  50,\n",
       "  24155,\n",
       "  4024,\n",
       "  311,\n",
       "  279,\n",
       "  15197,\n",
       "  13,\n",
       "  15469,\n",
       "  4024,\n",
       "  311,\n",
       "  279,\n",
       "  14150,\n",
       "  13,\n",
       "  10455,\n",
       "  4024,\n",
       "  311,\n",
       "  279,\n",
       "  13863,\n",
       "  13,\n",
       "  56786,\n",
       "  7882,\n",
       "  311,\n",
       "  279,\n",
       "  51902,\n",
       "  13,\n",
       "  11208,\n",
       "  374,\n",
       "  56786,\n",
       "  30,\n",
       "  22559,\n",
       "  449,\n",
       "  264,\n",
       "  3254,\n",
       "  3492,\n",
       "  13,\n",
       "  128009,\n",
       "  128006,\n",
       "  78191,\n",
       "  128007,\n",
       "  271,\n",
       "  43341,\n",
       "  3195,\n",
       "  128009],\n",
       " 'labels_mask': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_b['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class DatasetMix:\n",
    "    def __init__(self, datasets, ratios=None, random_seed=42):\n",
    "        self.datasets = datasets \n",
    "        self.gen = np.random.default_rng(seed=random_seed)\n",
    "        \n",
    "        if ratios is None:\n",
    "            ratios = [1/len(datasets) for _ in datasets]\n",
    "        self.ratios = ratios\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        dataset_ind = self.gen.choice(len(self.datasets), p=self.ratios)\n",
    "        dataset = self.datasets[dataset_ind]\n",
    "        return dataset[ind]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return min([len(d) for d in self.datasets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolly = load_dataset('databricks/databricks-dolly-15k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb67eaae4c3476bb1750323230bb2f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2k.json:   0%|          | 0.00/34.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b47ecb8cc4f4608ba81a91d1edf92cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2k.json:   0%|          | 0.00/34.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d804deb63f4a61ab159cafc657acfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2k.json:   0%|          | 0.00/35.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9179ad2b8895484b8948c9ca47c35868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2k.json:   0%|          | 0.00/34.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1507533cad14347a8188c15272157f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2k.json:   0%|          | 0.00/34.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7fc275ac5614558968ac631301e4b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2k.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480fb6c6a0204fbc8b18925c9cb52861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2k.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25e1f9290d94147a8206d55bee098b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2k.json:   0%|          | 0.00/34.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b2c0a5317641b2898ca16a4edf221d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2k.json:   0%|          | 0.00/34.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7763d399bffd4bfd9bcd9d1275baaf56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2k.json:   0%|          | 0.00/34.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c87a085f36a4fadbabb008651d5d399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa1 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90f500f5b414e638aca867131e6a87d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa2 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a60c0d43f446a3a40f8ac61188dc69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa3 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a55ea199ae4190859a8c3fd4d14346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa4 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d8d2c0dc5b462b9728f06463dbcfae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa5 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1c709ddf2247528907e9f22ed94c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa6 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ad3d69aff884faeb23cdf04c1b822aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa7 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8eec59471a544c697955933ba1bc64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa8 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d7db76dd8e94f8a92f0b45264832211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa9 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671d8040903042b4b2fa818584f3bd85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa10 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95f8db4678c453096e363085ce29cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "4k.json:   0%|          | 0.00/74.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4cbcd6135fe446d82b9a3c4b4b3a9e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "4k.json:   0%|          | 0.00/74.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2103210fbf5494faf55dc0e74e3c862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "4k.json:   0%|          | 0.00/75.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772a3d100c9e4100af12ca53d632bd28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "4k.json:   0%|          | 0.00/74.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91020247b8164b09815572e02ea2948a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "4k.json:   0%|          | 0.00/74.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deec98c4e6f945c1abfcafdad76633e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "4k.json:   0%|          | 0.00/74.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9df17c10a5489596af46ffa56a03d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "4k.json:   0%|          | 0.00/74.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346e02a099d046a4a58bae1e3739b80b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "4k.json:   0%|          | 0.00/74.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fb339c4471c44bfb849538f916e3a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "4k.json:   0%|          | 0.00/74.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873dfd517361449cb201b720e8d11952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "4k.json:   0%|          | 0.00/74.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e70597b1e9e41e98a8d69910b27f5e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa1 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40bd81dce52a4032b70996c8157d5d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa2 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef39a157c0bf4b11a60e31e75b9fe4a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa3 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d0e43a95c14b86b53669ea01eddb5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa4 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8073cd95a943fca0f3912b8fa99583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa5 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20af85fbe4b64bf084fef45bf3f759a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa6 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835e719cd04b4e1882d270c0c5554c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa7 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a63553b3d04ad2872df9aa4f4700f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa8 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a160142593a40f5947a1d8bc96b8987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa9 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2883be022f734a83bed9146ff81a8216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa10 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ebd89f9d69545248a0995650eed47fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "8k.json:   0%|          | 0.00/154M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bef9d07c02b4016bd34ed8959f49325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "8k.json:   0%|          | 0.00/154M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a32e67432a64a578c25fd8cbbd8a2a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "8k.json:   0%|          | 0.00/155M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15012d29879f4f0ab46a00c0c1dc4f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "8k.json:   0%|          | 0.00/154M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2c41872445448e9523143468779ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "8k.json:   0%|          | 0.00/154M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69374240dcd749e39eabd38086f741e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "8k.json:   0%|          | 0.00/155M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e4bbed9c2d49b8bba9ef3ac387697a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "8k.json:   0%|          | 0.00/154M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a8101ede0ee442b9e68d336f93becc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "8k.json:   0%|          | 0.00/155M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc235983045459e8e904c6d4c9b3777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "8k.json:   0%|          | 0.00/155M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c389198238ff4054bad6272d88a6ef25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "8k.json:   0%|          | 0.00/154M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbcdb1ffa37d41668b07028270ef5e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa1 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d7a021f3214f41b12e44a263f46d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa2 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09cafa9984d4a738ffacb9cb524cecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa3 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa99ddf1fc843e79cff75fa15f2e5e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa4 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b89f8c2be14f27ad1e70416d354b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa5 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c458315b8e5b4060bcdebec0260d41a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa6 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbcd65e33cbe46d8b3f8ff4e63441a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa7 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9348c9c08b0e4fbc8ffcaf9a2cb377ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa8 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24e4d922928416e93b2d60899c4ac32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa9 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08769f9d348c4556bece837617c13615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa10 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8faef99b5c43b69de7f591c7e4f638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "16k.json:   0%|          | 0.00/316M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb51a1f675a148febb39c70e830e975c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "16k.json:   0%|          | 0.00/316M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9809fb71e346c99755896adac4d22a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "16k.json:   0%|          | 0.00/317M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee0a5eea38a440892ae8301556cd746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "16k.json:   0%|          | 0.00/317M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14bfe39e9b954ea7b0212969b8165722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "16k.json:   0%|          | 0.00/316M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a361c8226644f69709436c19f664cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "16k.json:   0%|          | 0.00/316M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04931db696454b19ae396d3d24c88896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "16k.json:   0%|          | 0.00/316M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330d3f1d6dc746b09843db801b6bd992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "16k.json:   0%|          | 0.00/316M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3376cbc7454744b0ade620030c8e3f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "16k.json:   0%|          | 0.00/315M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c36303766c44cf8501ac8970c5f718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "16k.json:   0%|          | 0.00/316M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75de695dfca24bc19e72540f1aaae7fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa1 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73ceddd62fd4d3cad747f7b593963c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa2 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c1018f6b394b8da954b9fa24446a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa3 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4173c22938bf445391b0d2aff14d503c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa4 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be9e9c14afb4e87a4b35ff5196c5a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa5 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7243de14f141c6b5b1311090bd8069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa6 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7550de35853474bafe2c9dc66def180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa7 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ddf3bb18734ebea03261a34307ef49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa8 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce07f8d50ee49fe9df5593410f75cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa9 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9926d2223af6401aa29cf85a20114b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating qa10 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tasks = ['qa1', 'qa2', 'qa3', 'qa4', 'qa5']\n",
    "splits = ['0k', '2k', '4k', '8k', '16k']\n",
    "\n",
    "# for t in tasks:\n",
    "for s in splits:\n",
    "    d = load_dataset(\"RMT-team/babilong-train-5k-samples\", s)\n",
    "    de = load_dataset(\"RMT-team/babilong\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15011"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dolly['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'input', 'target'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1.select(range(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def combine_datasets(datasets_, ratios, target_size):\n",
    "    target_sizes = [int(target_size * r) for r in ratios]\n",
    "    datasets_scaled = []\n",
    "    for dataset, target_size in zip(datasets_, target_sizes):\n",
    "        if len(dataset) >= target_size:\n",
    "            dataset_scaled = dataset.select(range(target_size))\n",
    "        else:\n",
    "            repeated_dataset = [dataset] * (int(target_size / len(dataset)) + 1)\n",
    "            dataset_scaled = datasets.concatenate_datasets(repeated_dataset).select(range(target_size))\n",
    "        datasets_scaled.append(dataset_scaled)\n",
    "    return datasets.concatenate_datasets(datasets_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'context', 'response', 'category', 'question', 'input', 'target'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "target_size = 10_000\n",
    "datasets_ = [dolly['train'], d1, d2]\n",
    "ratios = [0.5, 0.25, 0.25]\n",
    "\n",
    "combine_datasets(datasets_, ratios, target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'When did Virgin Australia start operating?',\n",
       " 'context': \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\",\n",
       " 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.',\n",
       " 'category': 'closed_qa',\n",
       " 'question': None,\n",
       " 'input': None,\n",
       " 'target': None}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "babilong_datasets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'When did Virgin Australia start operating?',\n",
       " 'context': \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\",\n",
       " 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.',\n",
       " 'category': 'closed_qa'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dolly['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'When did Virgin Australia start operating?',\n",
       " 'context': \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\",\n",
       " 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.',\n",
       " 'category': 'closed_qa'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dolly['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'input', 'target'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = load_dataset('RMT-team/babilong', '0k')['qa7']\n",
    "d2 = load_dataset('RMT-team/babilong', '1k')['qa1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DatasetMix([d1, d2], ratios=[0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Where is Sandra? ',\n",
       " 'input': 'Sandra travelled to the garden. The usual\\npremonitory phenomena of suffocation will indicate the mistake. There\\nis some likelihood, too, of entering the larynx in individuals with\\nunusually prominent cervical vertebrae and in cases of stricture at the\\nextreme upper portion of the oesophagus. In introducing these\\ninstruments into the oesophagus, therefore, it is well that they be\\nguided along the fore finger of the disengaged hand, and passed deeply\\ninto the throat, either to the side of the larynx or behind it. John travelled to the kitchen. By\\nkeeping to the side and reaching the oesophagus by way of the\\nlaryngo-pharyngeal sinus the risk of entering the larynx may be\\navoided. Before introducing the tube the case should be carefully\\nexamined for aneurism, which by pressure sometimes gives rise to the\\nordinary subjective symptoms of stricture. Should aneurism be detected,\\npassage of the tube would be hazardous. PROGNOSIS.--The prognosis is in most instances unfavorable. It is\\ncomparatively favorable in cases of moderate stricture due to causes\\napparently remediable. The extent and volume of the stricture progress\\nmore or less slowly according to the nature of its cause, and in\\nnon-malignant cases, such as are due to the action of caustic\\nsubstances, it may last for years before the patient, if not relieved,\\nsuccumbs, as he does, from gradual inanition. In the earlier stages,\\nbefore the hypertrophied muscles above the stricture undergo fatty\\nmetamorphosis, the increased muscular power is sufficient to force\\nnourishment through the stricture; but when this becomes no longer\\npossible progressive marasmus must ensue. Mary went to the bathroom. Meantime, abscess may become\\ndeveloped in consequence of the pressure of retained food, and\\ntuberculous degeneration of the lung and local gangrene may take place\\nin consequence of the malnutrition. TREATMENT.--The treatment of organic stricture of the oesophagus\\nresolves itself into maintenance of the general health, the\\nadministration of the iodides to promote absorption of effusions into\\nthe connective tissue or the muscles, mechanical and operative measures\\nfor removal of the causes of the constriction or the strictured tissues\\nthemselves, and operations for securing artificial openings below the\\npoint of stricture for the introduction of nourishment (oesophagostomy\\nand gastrostomy). Nourishment by enema is of great value. Mary travelled to the bedroom. In carcinomatous stricture local measures are in the main\\nunjustifiable, as they usually entail injury which may prove very\\nserious. Arsenic internally is thought to <DW44> the progress of\\nmalignant disease when administered early and persistently. Morphine is\\nused hypodermically to assuage pain. In cancerous and tuberculous disease great caution is requisite in\\ndetermining upon mechanical or surgical procedures. In cicatricial\\nstenosis from the effects of caustic substances, such measures may be\\nundertaken with much less consideration. The local treatment consists in systematic mechanical dilatation with\\nbougies or',\n",
       " 'target': 'garden'}"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "args = Holder()\n",
    "args.task_dataset = \"qa1_single-supporting-fact\"\n",
    "# args.max_n_facts = 50\n",
    "args.vary_n_segments = False\n",
    "args.max_n_segments = 128\n",
    "args.segment_size = 1024\n",
    "args.sample_size = 512\n",
    "args.segment_alignment = 'right'\n",
    "\n",
    "base_model_path = \"/home/jovyan/kuratov/models/Llama-3.2-1B-Instruct/\"\n",
    "dtype = torch.bfloat16\n",
    "device = 'auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequence\n\u001b[0;32m----> 2\u001b[0m id_pad_value \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39meos_token_id\n\u001b[1;32m      3\u001b[0m gen_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGEN\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m eos_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token_id\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "id_pad_value = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "gen_token = tokenizer.encode('GEN')[0]\n",
    "eos_token = tokenizer.eos_token_id\n",
    "\n",
    "def get_input_ids(sample):\n",
    "    template = \"{} {}Answer with a single word.\"\n",
    "    context = tokenizer.decode(sample['input_tokens'])\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": template.format(context, sample['question'])},\n",
    "        {\"role\": \"assistant\", \"content\": sample['answer']}\n",
    "    ]\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    input_ids_short = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": template.format(context, sample['question'])}],\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    labels_mask = torch.zeros(len(input_ids))\n",
    "    labels_mask[len(input_ids_short) + 1:] = True\n",
    "    return input_ids, labels_mask\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = [get_input_ids(sample) for sample in batch]\n",
    "    input_ids = [torch.tensor(i[0]) for i in inputs]\n",
    "    labels_mask = [torch.tensor(i[1]) for i in inputs]\n",
    "    attention_mask = [torch.ones_like(b, dtype=bool) for b in input_ids]\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, padding_value=id_pad_value, batch_first=True)\n",
    "    attention_mask = pad_sequence(attention_mask, padding_value=0, batch_first=True)\n",
    "    labels_mask = pad_sequence(labels_mask, padding_value=0, batch_first=True)\n",
    "\n",
    "    collated = {}\n",
    "    collated['input_ids'] = collated['labels'] = input_ids\n",
    "    collated['labels_mask'] = labels_mask.bool()\n",
    "    collated['attention_mask'] = attention_mask.bool()\n",
    "    \n",
    "    return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
