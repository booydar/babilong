{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from huggingface_hub import PyTorchModelHubMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from original ARTM repo: https://raw.githubusercontent.com/RodkinIvan/associative-recurrent-memory-transformer/refs/heads/framework_accel/modeling_amt/language_modeling.py\n",
    "import math\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "from torch.nn.functional import relu as r\n",
    "\n",
    "def dpfp(x, nu=1):\n",
    "  x = torch.cat([r(x), r(-x)], dim=-1)\n",
    "  x_rolled = torch.cat([x.roll(shifts=j, dims=-1)\n",
    "           for j in range(1,nu+1)], dim=-1)\n",
    "  x_repeat = torch.cat([x] * nu, dim=-1)\n",
    "  return x_repeat * x_rolled\n",
    "\n",
    "class DPFP:\n",
    "    def __init__(self, nu):\n",
    "        self.nu = nu\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        nu = self.nu\n",
    "        x = torch.cat([r(x), r(-x)], dim=-1)\n",
    "        x_rolled = torch.cat([x.roll(shifts=j, dims=-1) for j in range(1,nu+1)], dim=-1)\n",
    "        x_repeat = torch.cat([x] * nu, dim=-1)\n",
    "        return x_repeat * x_rolled\n",
    "\n",
    "class AssociativeLayerWrapper(torch.nn.Module, PyTorchModelHubMixin):\n",
    "\n",
    "    def __init__(self, layer, d_model, num_mem_tokens, d_mem, correction=True, info=None) -> None:\n",
    "        super().__init__()\n",
    "        self.info = info\n",
    "        self.seg_num = 0\n",
    "        self.d_model = d_model\n",
    "        self.num_mem_tokens = num_mem_tokens\n",
    "        self.d_mem = d_mem\n",
    "\n",
    "        nu = 3\n",
    "        self.d_key = 2 * nu * d_mem\n",
    "        self.phi = DPFP(nu)\n",
    "        # self.d_key = d_mem\n",
    "        # self.phi = torch.nn.Identity()\n",
    "\n",
    "        self.W_mq = torch.nn.Linear(d_model, d_mem, bias=False, dtype=torch.bfloat16)\n",
    "        # torch.nn.init.zeros_(self.W_mq.weight)\n",
    "        self.W_mk = torch.nn.Linear(d_model, d_mem, bias=False, dtype=torch.bfloat16)\n",
    "        self.W_mv = torch.nn.Linear(d_model, d_model, bias=False, dtype=torch.bfloat16)\n",
    "        torch.nn.init.zeros_(self.W_mv.weight)\n",
    "        self.W_mb = torch.nn.Linear(d_model, 1, dtype=torch.bfloat16)\n",
    "\n",
    "        self.W_mem = torch.zeros(1, self.d_key, d_model, dtype=torch.bfloat16)\n",
    "        self.z = torch.zeros(1, self.d_key, dtype=torch.bfloat16)\n",
    "        self.W_mem.requires_grad_(False)\n",
    "        self.z.requires_grad_(False)\n",
    "        \n",
    "        # self.ln = torch.nn.LayerNorm(d_model)\n",
    "\n",
    "        self.zero_mem()\n",
    "    \n",
    "        self.layer = layer\n",
    "        \n",
    "        self.generate_mode = False\n",
    "        self.first_seg = True\n",
    "        self.correction = correction\n",
    "\n",
    "    def associate(self, hidden_states):\n",
    "\n",
    "        self.W_mem = self.W_mem.to(hidden_states.device).to(torch.bfloat16)\n",
    "        self.z = self.z.to(hidden_states.device).to(torch.bfloat16)\n",
    "        \n",
    "        mq = self.phi(self.W_mq(hidden_states)).to(torch.bfloat16) # (bsz, seq_len, 2d_mem * nu)\n",
    "\n",
    "        # crutch for dataparallel\n",
    "        # mq += 0 * self.W_mb(hidden_states).sum() * self.W_mk(hidden_states).sum() * self.W_mv(hidden_states).sum() \n",
    "        #print(mq, self.W_mem)\n",
    "        #print(mq.dtype, self.W_mem.dtype)\n",
    "        num = torch.einsum('ijk,ikt->ijt', mq, self.W_mem)\n",
    "        denom = torch.einsum(\"ik,ijk->ij\", self.z, mq)[..., None] + 1e-5\n",
    "        hidden_states = num / denom\n",
    "\n",
    "        return hidden_states\n",
    "    \n",
    "    def forward(self, hidden_states, **kwargs):\n",
    "        if not self.first_seg:\n",
    "            hidden_states = self.associate(\n",
    "                # self.ln(\n",
    "                    hidden_states\n",
    "                # )\n",
    "            ) + hidden_states\n",
    "        out = self.layer(hidden_states=hidden_states, **kwargs)\n",
    "        if not self.generate_mode:\n",
    "            mem_tokens = out[0][:, -self.num_mem_tokens:]\n",
    "            self.update_mem(mem_tokens)\n",
    "            self.first_seg = False\n",
    "        return out\n",
    "\n",
    "    def update_mem(self, mem_tokens):\n",
    "\n",
    "        self.W_mem = self.W_mem.to(mem_tokens.device)\n",
    "        self.z = self.z.to(mem_tokens.device)\n",
    "\n",
    "        mk = self.phi(self.W_mk(mem_tokens))\n",
    "        new_mv = self.W_mv(mem_tokens) # (bsz, num_mem_tokens, d_model)\n",
    "        if not self.first_seg:\n",
    "            num = torch.einsum('ijk,ikt->ijt', mk, self.W_mem)\n",
    "            denom = torch.einsum(\"ij,ikj->ik\", self.z, mk)[..., None] + 1e-5\n",
    "            prev_mv = num / denom\n",
    "            if self.correction:\n",
    "                new_info_coef = 1 - denom / (torch.linalg.norm(mk, dim=-1) ** 2 + 1e-5)[..., None]\n",
    "                new_info_coef = torch.clip(new_info_coef, 0, 1).detach()\n",
    "            else:\n",
    "                new_info_coef = 1\n",
    "        else: \n",
    "            prev_mv = torch.zeros_like(new_mv, device=new_mv.device)\n",
    "            new_info_coef = 1\n",
    "        \n",
    "        # wandb.log({f\"gamma_{self.info['layer']}\": new_info_coef.mean(dim=1).item() if isinstance(new_info_coef, torch.Tensor) else 1}, step=self.seg_num)\n",
    "        mv = new_mv - prev_mv\n",
    "\n",
    "        # new_norm = torch.linalg.norm(new_mv, dim=-1)\n",
    "        # old_norm = torch.linalg.norm(prev_mv, dim=-1)\n",
    "        # new_info_coef = torch.clip(1 - old_norm / (new_norm + 1e-5), -10, 10)[..., None].detach()\n",
    "        # new_info_coef = 1 - denom\n",
    "\n",
    "        mb = torch.sigmoid(self.W_mb(mem_tokens))[..., 0]\n",
    "\n",
    "        associations =  torch.einsum('ijk,ijt,ij->ikt', mk, mv, mb) # (bsz, d_mem, d_model)\n",
    "        self.W_mem = self.W_mem + associations\n",
    "\n",
    "        self.z = self.z + (new_info_coef*mk).sum(dim=1)\n",
    "        # self.z = self.z + (new_info_coef*mb[..., None]*mk).sum(dim=1)\n",
    "        self.seg_num += 1\n",
    "\n",
    "\n",
    "    def zero_mem(self):\n",
    "        self.first_seg = True\n",
    "        self.W_mem = torch.zeros(1, self.d_key, self.d_model)\n",
    "        self.z = torch.zeros(1, self.d_key)\n",
    "        self.seg_num = 0\n",
    "\n",
    "\n",
    "\n",
    "class AssociativeMemoryCell(torch.nn.Module, PyTorchModelHubMixin):\n",
    "    def __init__(self, base_model, num_mem_tokens, d_mem, layers_attr: str = 'transformer.h', wrap_pos=True, correction=True, use_lora=False):\n",
    "        super().__init__()\n",
    "        self.model = base_model\n",
    "        self.num_mem_tokens = num_mem_tokens\n",
    "        self.d_mem = d_mem\n",
    "        self.d_model = base_model.get_input_embeddings().embedding_dim\n",
    "        self.W_mq = torch.nn.ModuleList()\n",
    "        self.W_mem = []\n",
    "        if use_lora:\n",
    "            # LoRA case\n",
    "            self.layers = self.model.model\n",
    "        else:\n",
    "            self.layers = self.model\n",
    "\n",
    "        self.layers_attrs = layers_attr.split('.')\n",
    "        for i, attr in enumerate(self.layers_attrs):\n",
    "            self.layers = getattr(self.layers, attr)\n",
    "        \n",
    "        for i in range(len(self.layers)):\n",
    "            self.layers[i] = AssociativeLayerWrapper(\n",
    "                self.layers[i], \n",
    "                self.d_model, \n",
    "                self.num_mem_tokens, \n",
    "                self.d_mem, \n",
    "                correction,\n",
    "                info={'layer': i}\n",
    "            )\n",
    "        self.create_memory(num_mem_tokens)\n",
    "        self.wrap_pos = wrap_pos\n",
    "        if wrap_pos:\n",
    "            self.wrap_positional_embeddings(num_mem_tokens)\n",
    "    \n",
    "    def generate_mode(self, is_on):\n",
    "        for layer in self.layers:\n",
    "            layer.generate_mode = is_on\n",
    "    \n",
    "    def create_memory(self, num_mem_tokens):\n",
    "        self.num_mem_tokens = num_mem_tokens\n",
    "        embeddings = self.model.get_input_embeddings()\n",
    "        memory_dim =  getattr(self.model.config, 'n_embd', self.model.config.hidden_size)\n",
    "        memory_weights = torch.randn((num_mem_tokens, memory_dim)) * embeddings.weight.data.std()\n",
    "        self.register_parameter('memory', torch.nn.Parameter(memory_weights, requires_grad=True))\n",
    "\n",
    "    def wrap_positional_embeddings(self, num_mem_tokens):\n",
    "        num_pos_embs, emb_dim = self.model.transformer.wpe.weight.shape\n",
    "        prev_embs = self.model.transformer.wpe.weight.detach()\n",
    "        self.model.transformer.wpe = torch.nn.Embedding(num_mem_tokens + num_pos_embs, emb_dim)\n",
    "\n",
    "        new_num_pos = num_pos_embs + num_mem_tokens\n",
    "        with torch.no_grad():\n",
    "            self.model.transformer.wpe.weight[:len(self.model.transformer.wpe.weight)-num_mem_tokens] = prev_embs\n",
    "        for layer in self.model.transformer.h:\n",
    "            layer.layer.attn.bias = torch.tril(torch.ones((new_num_pos, new_num_pos), dtype=torch.uint8)).view(\n",
    "                1, 1, new_num_pos, new_num_pos\n",
    "            )\n",
    "\n",
    "    def set_memory(self, input_shape):\n",
    "        memory = self.memory.repeat(input_shape[0], 1, 1)\n",
    "        return memory\n",
    "\n",
    "    def zero_mem(self):\n",
    "        for layer in self.layers:\n",
    "            layer.zero_mem()\n",
    "\n",
    "    def forward(self, input_ids, labels=None, labels_mask=None, zero_mem=False, **kwargs):\n",
    "        if zero_mem:\n",
    "            self.zero_mem()\n",
    "\n",
    "\n",
    "        seg_kwargs = self.process_input(input_ids, **kwargs)\n",
    "\n",
    "        out = self.model(**seg_kwargs)\n",
    "\n",
    "        out = self.process_output(out, labels, labels_mask, **kwargs)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def process_input(self, input_ids, **kwargs):\n",
    "        memory_state = self.set_memory(input_ids.shape)\n",
    "        seg_kwargs = dict(**kwargs)\n",
    "        inputs_embeds = kwargs.get('inputs_embeds')\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.model.get_input_embeddings()(input_ids)\n",
    "        inputs_embeds = torch.cat([inputs_embeds, memory_state], dim=1)\n",
    "\n",
    "        seg_kwargs['input_ids'] = None\n",
    "        seg_kwargs['inputs_embeds'] = inputs_embeds\n",
    "        if kwargs.get('attention_mask') is not None:\n",
    "            seg_kwargs['attention_mask'] = self.pad_attention_mask(kwargs['attention_mask'], inputs_embeds.shape)\n",
    "            if kwargs.get('prev_attn_mask') is not None:\n",
    "                seg_kwargs['attention_mask'] = torch.cat([kwargs['prev_attn_mask'], seg_kwargs['attention_mask']], dim=-1)\n",
    "        if 'prev_attn_mask' in seg_kwargs.keys():\n",
    "            seg_kwargs.pop('prev_attn_mask')\n",
    "        seg_kwargs['output_hidden_states'] = True\n",
    "\n",
    "        if self.wrap_pos:\n",
    "            num_pos_embs = self.model.transformer.wpe.weight.shape[0]\n",
    "            ordinary_pos = torch.arange(0, input_ids.size(1), dtype=torch.long, device=input_ids.device)\n",
    "            write_pos = torch.arange(num_pos_embs - self.num_mem_tokens, num_pos_embs, dtype=torch.long, device=input_ids.device)\n",
    "            seg_kwargs['position_ids'] = torch.cat([\n",
    "                ordinary_pos, \n",
    "                write_pos\n",
    "            ]).long().unsqueeze(0)\n",
    "        return seg_kwargs\n",
    "    \n",
    "    def pad_attention_mask(self, attention_mask, shape):\n",
    "        if self.num_mem_tokens in {0, None}:\n",
    "            return attention_mask\n",
    "        else:\n",
    "            mask = torch.ones(*shape[:2], dtype=torch.int64).to(attention_mask.device)\n",
    "            mask[:, :-self.num_mem_tokens] = attention_mask\n",
    "            return mask\n",
    "    \n",
    "    def process_output(self, model_outputs, labels, labels_mask, **kwargs):\n",
    "        if self.num_mem_tokens not in {0, None}:\n",
    "            out = CausalLMOutputWithCrossAttentions()\n",
    "            out['logits'] = model_outputs.logits[:, :-self.num_mem_tokens]\n",
    "            if kwargs.get('output_hidden_states'):\n",
    "                out['hidden_states'] = [lh[:, :-self.num_mem_tokens] for lh in model_outputs.hidden_states]\n",
    "            if kwargs.get('output_attentions'):\n",
    "                out['attentions'] = model_outputs['attentions']\n",
    "        else:\n",
    "            out = model_outputs\n",
    "\n",
    "        if labels is not None:\n",
    "            ce_loss_fn = CrossEntropyLoss()\n",
    "            logits = out['logits'][..., :-1, :].contiguous()\n",
    "            flat_logits = logits.view(-1, logits.size(-1))\n",
    "            labels = labels[..., 1:].contiguous()\n",
    "            flat_labels = labels.view(-1)\n",
    "            if labels_mask is not None:\n",
    "                flat_mask = labels_mask[..., :-1].contiguous().view(-1)\n",
    "\n",
    "                flat_logits = flat_logits[flat_mask]\n",
    "                flat_labels = flat_labels[flat_mask]\n",
    "            ce_loss = ce_loss_fn(flat_logits, flat_labels)\n",
    "            out['ce_loss'] = ce_loss\n",
    "\n",
    "        if kwargs.get('use_cache') is not None:\n",
    "            out['past_key_values'] = model_outputs.past_key_values\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def generate(self, input_ids, attention_mask, zero_mem=False, **generate_kwargs):\n",
    "        if zero_mem:\n",
    "            self.zero_mem()\n",
    "        \n",
    "        \n",
    "        self.generate_mode(True)\n",
    "        seg_kwargs = self.process_input(input_ids, attention_mask=attention_mask)\n",
    "        out = self.model.generate(\n",
    "            inputs_embeds=seg_kwargs['inputs_embeds'][:, :-self.num_mem_tokens], \n",
    "            attention_mask=seg_kwargs['attention_mask'][:, :-self.num_mem_tokens], \n",
    "            **generate_kwargs\n",
    "        )\n",
    "        self.generate_mode(False)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class AssociativeRecurrentWrapper(torch.nn.Module, PyTorchModelHubMixin):\n",
    "    def __init__(self, memory_cell, **rmt_kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.memory_cell = memory_cell\n",
    "        self.rmt_config = rmt_kwargs\n",
    "\n",
    "    def forward(self, \n",
    "                input_ids, \n",
    "                labels=None, \n",
    "                labels_mask=None, \n",
    "                inputs_embeds=None, \n",
    "                attention_mask=None, \n",
    "                output_attentions=None, \n",
    "                output_hidden_states=None,\n",
    "                input_segmented=False,\n",
    "                sliding_window=False,\n",
    "                ):\n",
    "        if input_segmented:\n",
    "            n_segs = input_ids.shape[1] if not (input_ids is None) else inputs_embeds.shape[1]\n",
    "            segmented = [dict(\n",
    "                input_ids=input_ids[:, i] if not (input_ids is None) else None, \n",
    "                inputs_embeds=inputs_embeds[:, i] if not (inputs_embeds is None) else None, \n",
    "                attention_mask=attention_mask[:, i],\n",
    "                labels=labels[:, i] if not (labels is None) else None, \n",
    "                labels_mask=labels_mask[:, i] if not (labels_mask is None) else None, \n",
    "            ) for i in range(n_segs)]\n",
    "            labels = torch.cat([labels[:, i] for i in range(n_segs)], dim=1)\n",
    "            if labels_mask is not None:\n",
    "                labels_mask = torch.cat([labels_mask[:, i] for i in range(n_segs)], dim=1)\n",
    "        else:\n",
    "            segmented = self.segment(input_ids=input_ids, inputs_embeds=inputs_embeds, attention_mask=attention_mask, labels=labels, labels_mask=labels_mask)\n",
    "        cell_outputs = []\n",
    "        past_key_values = None\n",
    "        num_mem_tokens = self.memory_cell.num_mem_tokens\n",
    "        prev_attn_mask = None\n",
    "        self.memory_cell.zero_mem()\n",
    "        for seg_num, segment in enumerate(segmented):\n",
    "            seg_len = segment['input_ids'].size(-1)\n",
    "            cell_out = self.memory_cell(**segment,  \n",
    "                                        output_hidden_states=True, \n",
    "                                        use_cache=sliding_window, \n",
    "                                        past_key_values=past_key_values,\n",
    "                                        prev_attn_mask=prev_attn_mask,\n",
    "                                        zero_mem=False\n",
    "            )\n",
    "            if sliding_window:\n",
    "                prev_attn_mask = segment['attention_mask']\n",
    "                past_key_values = [\n",
    "                    [\n",
    "                        k_or_v[..., -(num_mem_tokens+seg_len):k_or_v.size(-2)-num_mem_tokens, :].detach() \n",
    "                        for k_or_v in seg_kv\n",
    "                    ] \n",
    "                    for seg_kv in cell_out['past_key_values']\n",
    "                ]\n",
    "            cell_outputs.append(cell_out)\n",
    "        self.memory_cell.zero_mem()\n",
    "\n",
    "\n",
    "        out = self.process_outputs(cell_outputs, labels=labels, \n",
    "                                   labels_mask=labels_mask,\n",
    "                                   output_attentions=output_attentions, \n",
    "                                   output_hidden_states=output_hidden_states)\n",
    "        return out\n",
    "\n",
    "    def segment(self, **kwargs):\n",
    "        segments = []\n",
    "        for k, tensor in kwargs.items():\n",
    "            if tensor is not None:\n",
    "                k_segments = self.split_tensor(tensor)\n",
    "                for s, k_seg in enumerate(k_segments):\n",
    "                    if s < len(segments):\n",
    "                        segments[s][k] = k_seg\n",
    "                    else:\n",
    "                        segments.append({k: k_seg})\n",
    "\n",
    "        return segments\n",
    "    \n",
    "    def split_tensor(self, tensor):\n",
    "        align = self.rmt_config.get('segment_alignment')\n",
    "        segment_size = self.rmt_config.get('segment_size')\n",
    "        if align in {'left', None}:\n",
    "            split_inds = list(range(0, tensor.shape[1], segment_size)) + [tensor.shape[1]]\n",
    "            segments = [tensor[:, start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "        elif align in {'right', None}:\n",
    "            split_inds = (list(range(tensor.shape[1], 0, -segment_size)) + [0])[::-1]\n",
    "            segments = [tensor[:, start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "        elif align == 'center':\n",
    "            n_seg = math.ceil(tensor.shape[1] / segment_size)\n",
    "            segments = torch.chunk(tensor, n_seg, dim=1)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return segments\n",
    "\n",
    "    def process_outputs(self, cell_outputs, **kwargs):\n",
    "        out = CausalLMOutputWithCrossAttentions()\n",
    "        full_logits = torch.cat([o.logits for o in cell_outputs], dim=1)\n",
    "        full_hidden_states = tuple([torch.cat(layer_hs, dim=1) for layer_hs in zip(*[o.hidden_states for o in cell_outputs])])\n",
    "\n",
    "        labels = kwargs.get('labels')\n",
    "        if labels is not None:\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            shift_logits = full_logits[..., :-1, :].contiguous()\n",
    "            flat_labels = shift_labels.view(-1)\n",
    "            flat_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "            \n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            labels_mask = kwargs.get('labels_mask')\n",
    "            if labels_mask is not None:\n",
    "                shift_mask = labels_mask[..., :-1].contiguous()\n",
    "\n",
    "                flat_labels = flat_labels[shift_mask.view(-1)]\n",
    "                flat_logits = flat_logits[shift_mask.view(-1)]\n",
    "                \n",
    "            out['loss'] = loss_fct(flat_logits, flat_labels)\n",
    "        else:\n",
    "            out['loss'] = 0 \n",
    "\n",
    "        if self.rmt_config.get(\"return_all_logits\", False):\n",
    "            out['ce_loss'] = out['loss']\n",
    "        \n",
    "        out['logits'] = full_logits\n",
    "        segment_keys = ['loss', 'logits']\n",
    "        if kwargs.get('output_attentions'):\n",
    "            segment_keys.append('attentions')\n",
    "        if kwargs.get('output_hidden_states'):\n",
    "            segment_keys.append('hidden_states')\n",
    "            out['hidden_states'] = full_hidden_states\n",
    "\n",
    "        if self.rmt_config.get(\"return_all_logits\", False):\n",
    "            for seg_num, o in enumerate(cell_outputs):\n",
    "                for key, value in o.items():\n",
    "                    if any([sk in key for sk in segment_keys]):\n",
    "                        out[f'{key}_{seg_num}'] = value\n",
    "        return out \n",
    "        \n",
    "    def manage_gradients(self, memory_state, seg_num):\n",
    "        k2, max_n_segments = self.rmt_config.get('k2'), self.rmt_config.get('max_n_segments')\n",
    "        if seg_num == 0 \\\n",
    "            or k2 in {-1, None} \\\n",
    "            or seg_num + k2 > max_n_segments:\n",
    "                return True\n",
    "        \n",
    "        memory_state = memory_state.detach()\n",
    "        return False\n",
    "    \n",
    "    def generate(self, input_ids, attention_mask, **generate_kwargs):\n",
    "        self.memory_cell.zero_mem()\n",
    "        segmented = self.segment(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        for seg_num, segment in enumerate(segmented[:-1]):\n",
    "            cell_out = self.memory_cell(**segment, output_hidden_states=True, zero_mem=False)\n",
    "\n",
    "        final_segment = segmented[-1]\n",
    "        out = self.memory_cell.generate(**final_segment, zero_mem=False, **generate_kwargs)\n",
    "        self.memory_cell.zero_mem()\n",
    "        return out\n",
    "\n",
    "    def gradient_checkpointing_enable(self, *args, **kwargs):\n",
    "        # doesn't supported for ARMT\n",
    "        self.memory_cell.model.gradient_checkpointing_enable(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"/home/jovyan/kuratov/models/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    inference_mode=False, \n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.1\n",
    "    )\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_cell_args = dict(\n",
    "    base_model=model.cpu(),\n",
    "    num_mem_tokens=16,\n",
    ")\n",
    "mem_cell_args['d_mem'] = 64\n",
    "mem_cell_args['wrap_pos'] = False\n",
    "mem_cell_args['correction'] = False\n",
    "mem_cell_args['layers_attr'] = \"base_model.base_model.layers\"\n",
    "\n",
    "cell = AssociativeMemoryCell(**mem_cell_args)\n",
    "model = AssociativeRecurrentWrapper(cell, \n",
    "                                segment_size=1024,\n",
    "                                max_n_segments=128, \n",
    "                                vary_n_segments=False,\n",
    "                                k2=-1,\n",
    "                                return_all_logits=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c2781421c34587be426567eecad416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/booydar/RMT-Llama-3.2-1B-Instruct-8x1024-mem16-lora-babilong-qa1-5_ct-v3.1/commit/4be151ac42d7c70a59bbaffe7bd0a04b524eaa9f', commit_message='Push model using huggingface_hub.', commit_description='', oid='4be151ac42d7c70a59bbaffe7bd0a04b524eaa9f', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cpt_path = \"/home/jovyan/rmt/runs/test/babilong_multitask/meta-llama/Llama-3.2-1B-Instruct/lr_3e-04_d64_linear_adamw_wd1e-03_1x1024_mem16_bs64_bptt--1_from_cpt_0-1_lora/run_1/checkpoint-7500/pytorch_model.bin\"\n",
    "# cpt_path = \"/home/jovyan/rmt/runs/test/babilong_multitask/meta-llama/Llama-3.2-1B-Instruct/lr_3e-04_d64_linear_adamw_wd1e-03_2x1024_mem16_bs64_bptt--1_from_cpt_1-2_lora/run_1/checkpoint-8000/pytorch_model.bin\"\n",
    "# cpt_path = \"/home/jovyan/rmt/runs/test/babilong_multitask/meta-llama/Llama-3.2-1B-Instruct/lr_3e-04_d64_linear_adamw_wd1e-03_4x1024_mem16_bs64_bptt--1_from_cpt_2-4_lora/run_1/checkpoint-8000/pytorch_model.bin\"\n",
    "# cpt_path = \"/home/jovyan/rmt/runs/test/babilong_multitask/meta-llama/Llama-3.2-1B-Instruct/lr_3e-04_d64_linear_adamw_wd1e-03_4x1024_mem16_bs64_bptt--1_from_cpt_2-4_lora_ct/run_1/checkpoint-5000/pytorch_model.bin\"\n",
    "\n",
    "# checkpoints = [\n",
    "#     \"/home/jovyan/rmt/runs/test/babilong_multitask/meta-llama/Llama-3.2-1B-Instruct/lr_3e-04_d64_cosine_adamw_wd1e-03_1x1024_mem16_bs64_bptt--1_from_cpt_0-1_lora_ct-v3/run_1/checkpoint-6500/pytorch_model.bin\",\n",
    "#     \"/home/jovyan/rmt/runs/test/babilong_multitask/meta-llama/Llama-3.2-1B-Instruct/lr_3e-04_d64_cosine_adamw_wd1e-03_2x1024_mem16_bs64_bptt--1_from_cpt_1-2_lora_ct-v3/run_1/checkpoint-28000/pytorch_model.bin\",\n",
    "#     \"/home/jovyan/rmt/runs/test/babilong_multitask/meta-llama/Llama-3.2-1B-Instruct/lr_3e-04_d64_cosine_adamw_wd1e-03_4x1024_mem16_bs64_bptt--1_from_cpt_2-4_lora_ct-v3/run_1/checkpoint-24500/pytorch_model.bin\",\n",
    "#     \"/home/jovyan/rmt/runs/test/babilong_multitask/meta-llama/Llama-3.2-1B-Instruct/lr_3e-04_d64_cosine_adamw_wd1e-03_8x1024_mem16_bs64_bptt--1_from_cpt_4-8_lora_ct-v3/run_1/checkpoint-30000/pytorch_model.bin\",\n",
    "#     # \"/home/jovyan/rmt/runs/test/babilong_multitask/meta-llama/Llama-3.2-1B-Instruct/lr_3e-04_d64_cosine_adamw_wd1e-03_16x1024_mem16_bs64_bptt--1_from_cpt_8-16_lora_ct-v3/run_1/checkpoint-6000/pytorch_model.bin\",\n",
    "# ]\n",
    "\n",
    "# cpt_path = \"/home/jovyan/rmt/runs/test/babilong_multitask/meta-llama/Llama-3.2-1B-Instruct/lr_3e-04_d64_linear_adamw_wd1e-03_2x1024_mem16_bs64_bptt--1_from_cpt_1-2_lora_ct-v3/run_1/checkpoint-15500/pytorch_model.bin\"\n",
    "# model_name = \"booydar/RMT-Llama-3.2-1B-Instruct-2x1024-mem16-lora-babilong-qa1-5_ct-v3\"\n",
    "\n",
    "# cpt_path = \"/home/jovyan/rmt/runs/test/babilong_multitask/meta-llama/Llama-3.2-1B-Instruct/lr_3e-04_d64_cosine_adamw_wd1e-03_8x1024_mem16_bs64_bptt--1_from_cpt_4-8_lora_ct-v3/run_1/checkpoint-30000/pytorch_model.bin\"\n",
    "cpt_path  = \"/home/jovyan/rmt/runs/test//Llama-3.2-1B-Instruct/dolly:qa1-5-1:9/SEGM_2x1024_2016_64-lora-mnc-distill_1.0/checkpoint-1000/pytorch_model.bin\"\n",
    "\n",
    "# model_name = \"booydar/RMT-Llama-3.2-1B-Instruct-8x1024-mem16-lora-babilong-qa1-5_ct-v3.1\"\n",
    "model_name = \"booydar/RMT-Llama-3.2-1B-Instruct-2x1024-mem16-lora-dolly:qa1-5-1:9-qa1-5-distill_1.0-1000_steps\"\n",
    "\n",
    "\n",
    "cpt = torch.load(cpt_path, map_location='cpu')\n",
    "model.load_state_dict(cpt)\n",
    "\n",
    "model.push_to_hub(model_name, token=\"???\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4418950c1043a1915b5f35c49ca863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/booydar/RMT-Llama-3.2-1B-Instruct-4x1024-mem16-lora-babilong-qa1-5_ct-v3/commit/4d1076a8e72bd190168bd4a08cf1c46e6befe6d7', commit_message='Push model using huggingface_hub.', commit_description='', oid='4d1076a8e72bd190168bd4a08cf1c46e6befe6d7', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpt_path = \"/home/jovyan/rmt/runs/test/babilong_multitask/meta-llama/Llama-3.2-1B-Instruct/lr_3e-04_d64_linear_adamw_wd1e-03_4x1024_mem16_bs64_bptt--1_from_cpt_2-4_lora_ct-v3/run_1/checkpoint-2500/pytorch_model.bin\"\n",
    "model_name = \"booydar/RMT-Llama-3.2-1B-Instruct-4x1024-mem16-lora-babilong-qa1-5_ct-v3\"\n",
    "\n",
    "cpt = torch.load(cpt_path, map_location='cpu')\n",
    "model.load_state_dict(cpt)\n",
    "\n",
    "model.push_to_hub(model_name, token=\"???\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpt_path = \"/home/jovyan/rmt/runs/pg19/llama32-1b/linear_adamw_wd1e-03_4x1024_mem16_bs256_bptt--1_nfs/run_1/checkpoint-31500/pytorch_model.bin\"\n",
    "model_name = \"AIRI-NLP/RMT-Llama-3.2-1B-4x1024-mem16-pg19-31k_it\"\n",
    "\n",
    "cpt = torch.load(cpt_path, map_location='cpu')\n",
    "rmt.load_state_dict(cpt, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68fc74aa42ff4c438b64be5d734eb03d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/AIRI-NLP/RMT-Llama-3.2-1B-4x1024-mem16-pg19-31k_it/commit/a59ccd8945a89083c73d2239b44c06f682096c6f', commit_message='Push model using huggingface_hub.', commit_description='', oid='a59ccd8945a89083c73d2239b44c06f682096c6f', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt.push_to_hub(model_name, token=\"???\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "\n",
    "\n",
    "class MyModel(\n",
    "    nn.Module,\n",
    "    PyTorchModelHubMixin, \n",
    "    # optionally, you can add metadata which gets pushed to the model card\n",
    "    repo_url=\"your-repo-url\",\n",
    "    pipeline_tag=\"text-to-image\",\n",
    "    license=\"mit\",\n",
    "):\n",
    "    def __init__(self, num_channels: int, hidden_size: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.param = nn.Parameter(torch.rand(num_channels, hidden_size))\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x + self.param)\n",
    "\n",
    "# create model\n",
    "config = {\"num_channels\": 3, \"hidden_size\": 32, \"num_classes\": 10}\n",
    "model = MyModel(**config)\n",
    "\n",
    "# save locally\n",
    "model.save_pretrained(\"my-awesome-model\")\n",
    "\n",
    "# push to the hub\n",
    "model.push_to_hub(\"your-hf-username/my-awesome-model\")\n",
    "\n",
    "# reload\n",
    "model = MyModel.from_pretrained(\"your-hf-username/my-awesome-model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
