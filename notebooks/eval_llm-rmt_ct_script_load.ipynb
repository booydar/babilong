{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "## Run model and collect predictions on BABILong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import datasets\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from babilong.prompts import DEFAULT_PROMPTS, DEFAULT_TEMPLATE, get_formatted_input\n",
    "from babilong.babilong_utils import compare_answers\n",
    "\n",
    "from modeling_amt.language_modeling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "args = Holder()\n",
    "args.task_dataset = \"qa1_single-supporting-fact\"\n",
    "# args.max_n_facts = 50\n",
    "args.vary_n_segments = False\n",
    "args.max_n_segments = 128\n",
    "args.segment_size = 1024\n",
    "args.sample_size = 512\n",
    "args.segment_alignment = 'right'\n",
    "\n",
    "# base_model_path = \"unsloth/Llama-3.2-1B-Instruct\"#\"/home/jovyan/kuratov/models/Llama-3.2-1B-Instruct/\"\n",
    "base_model_path = \"/home/jovyan/kuratov/models/Llama-3.2-1B-Instruct/\"\n",
    "#base_model_path = \"meta-llama/Llama-3.2-1B\"\n",
    "dtype = torch.bfloat16\n",
    "device = 'auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_path, trust_remote_code=True,\n",
    "                                             device_map=device, \n",
    "                                             torch_dtype=dtype,\n",
    "                                             use_cache=False,\n",
    "                                             attn_implementation='flash_attention_2')\n",
    "if base_model_path == \"meta-llama/Llama-3.2-1B\":\n",
    "    it_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", trust_remote_code=True)\n",
    "    tokenizer.chat_template = it_tokenizer.chat_template\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    inference_mode=False, \n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.1\n",
    "    )\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AssociativeRecurrentWrapper' object has no attribute 'get_input_embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m mem_cell_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrection\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      8\u001b[0m mem_cell_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayers_attr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_model.base_model.layers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m cell \u001b[38;5;241m=\u001b[39m \u001b[43mAssociativeMemoryCell\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmem_cell_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m AssociativeRecurrentWrapper(cell, \n\u001b[1;32m     12\u001b[0m                                 segment_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39msegment_size,\n\u001b[1;32m     13\u001b[0m                                 max_n_segments\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmax_n_segments, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m                                 return_all_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     18\u001b[0m )\n",
      "File \u001b[0;32m~/rmt/babilong/modeling_amt/language_modeling.py:147\u001b[0m, in \u001b[0;36mAssociativeMemoryCell.__init__\u001b[0;34m(self, base_model, num_mem_tokens, d_mem, layers_attr, wrap_pos, correction, use_lora)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_mem_tokens \u001b[38;5;241m=\u001b[39m num_mem_tokens\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_mem \u001b[38;5;241m=\u001b[39m d_mem\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model \u001b[38;5;241m=\u001b[39m \u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_input_embeddings\u001b[49m()\u001b[38;5;241m.\u001b[39membedding_dim\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_mq \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleList()\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_mem \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/envs/ayd_py3.11_pt2_cu11.8/lib/python3.11/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AssociativeRecurrentWrapper' object has no attribute 'get_input_embeddings'"
     ]
    }
   ],
   "source": [
    "mem_cell_args = dict(\n",
    "    base_model=model.cpu(),\n",
    "    num_mem_tokens=16,\n",
    ")\n",
    "mem_cell_args['d_mem'] = 64\n",
    "mem_cell_args['wrap_pos'] = False\n",
    "mem_cell_args['correction'] = False\n",
    "mem_cell_args['layers_attr'] = \"base_model.base_model.layers\"\n",
    "\n",
    "cell = AssociativeMemoryCell(**mem_cell_args)\n",
    "model = AssociativeRecurrentWrapper(cell, \n",
    "                                segment_size=args.segment_size,\n",
    "                                max_n_segments=args.max_n_segments, \n",
    "                                vary_n_segments=args.vary_n_segments,\n",
    "                                segment_alignment='right',\n",
    "                                k2=-1,\n",
    "                                return_all_logits=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls /home/jovyan/rmt/runs/test/babilong_multitask/meta-llama/Llama-3.2-1B-Instruct/lr_3e-04_d64_linear_adamw_wd1e-03_4x1024_mem16_bs64_bptt--1_from_cpt_2-4_lora_ct-v3/run_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpt_path = \"/home/jovyan/rmt/runs/test/babilong_multitask/meta-llama/Llama-3.2-1B-Instruct/lr_3e-04_d64_linear_adamw_wd1e-03_1x1024_mem16_bs64_bptt--1_from_cpt_0-1_lora/run_1/checkpoint-7500/pytorch_model.bin\"\n",
    "# cpt_path = \"/home/jovyan/rmt/runs/test/babilong_multitask/meta-llama/Llama-3.2-1B-Instruct/lr_3e-04_d64_linear_adamw_wd1e-03_2x1024_mem16_bs64_bptt--1_from_cpt_1-2_lora/run_1/checkpoint-8000/pytorch_model.bin\"\n",
    "# cpt_path = \"/home/jovyan/rmt/runs/test/babilong_multitask/meta-llama/Llama-3.2-1B-Instruct/lr_3e-04_d64_linear_adamw_wd1e-03_4x1024_mem16_bs64_bptt--1_from_cpt_2-4_lora/run_1/checkpoint-8000/pytorch_model.bin\"\n",
    "# cpt_path = \"/home/jovyan/rmt/runs/test/babilong_multitask/meta-llama/Llama-3.2-1B-Instruct/lr_3e-04_d64_linear_adamw_wd1e-03_4x1024_mem16_bs64_bptt--1_from_cpt_2-4_lora_ct/run_1/checkpoint-5000/pytorch_model.bin\"\n",
    "# cpt_path = \"/home/jovyan/rmt/runs/test/babilong_multitask/meta-llama/Llama-3.2-1B-Instruct/lr_3e-04_d64_linear_adamw_wd1e-03_4x1024_mem16_bs64_bptt--1_from_cpt_2-4_lora_ct-v2/run_1/checkpoint-2500/pytorch_model.bin\"\n",
    "# cpt_path = \"/home/jovyan/rmt/runs/test/babilong_multitask/meta-llama/Llama-3.2-1B-Instruct/lr_3e-04_d64_linear_adamw_wd1e-03_2x1024_mem16_bs64_bptt--1_from_cpt_1-2_lora_ct-v3/run_1/checkpoint-15500/pytorch_model.bin\"\n",
    "# checkpoints = [\n",
    "#     \"/home/jovyan/rmt/runs/test/babilong_multitask/meta-llama/Llama-3.2-1B-Instruct/lr_3e-04_d64_linear_adamw_wd1e-03_1x1024_mem16_bs64_bptt--1_from_cpt_0-1_lora_ct-v3/run_1/checkpoint-6500/pytorch_model.bin\",\n",
    "#     \"/home/jovyan/rmt/runs/test/babilong_multitask/meta-llama/Llama-3.2-1B-Instruct/lr_3e-04_d64_linear_adamw_wd1e-03_2x1024_mem16_bs64_bptt--1_from_cpt_1-2_lora_ct-v3/run_1/checkpoint-15500/pytorch_model.bin\",\n",
    "#     \"/home/jovyan/rmt/runs/test/babilong_multitask/meta-llama/Llama-3.2-1B-Instruct/lr_3e-04_d64_linear_adamw_wd1e-03_4x1024_mem16_bs64_bptt--1_from_cpt_2-4_lora_ct-v3/run_1/checkpoint-2500/pytorch_model.bin\",\n",
    "#     \"/home/jovyan/rmt/runs/test/babilong_multitask/meta-llama/Llama-3.2-1B-Instruct/lr_3e-04_d64_linear_adamw_wd1e-03_8x1024_mem16_bs64_bptt--1_from_cpt_4-8_lora_ct-v3/run_1/checkpoint-9000/pytorch_model.bin\",\n",
    "#     \"/home/jovyan/rmt/runs/test/babilong_multitask/meta-llama/Llama-3.2-1B-Instruct/lr_3e-04_d64_linear_adamw_wd1e-03_16x1024_mem16_bs64_bptt--1_from_cpt_8-16_lora_ct-v3/run_1/checkpoint-6000/pytorch_model.bin\",\n",
    "\n",
    "# ]\n",
    "# MODEL_CPT='/home/jovyan/rmt_it/data/pretrained_models/RMT-Llama-3.2-1B-Instruct-4x1024-mem16-lora-babilong-qa1-5_ct-v3/model.safetensors'\n",
    "\n",
    "# checkpoints = [\n",
    "#     '/home/jovyan/gkuzmin/rmt_it/data/pretrained_models/RMT-Llama-3.2-1B-Instruct-4x1024-mem16-lora-babilong-qa1-5_ct-v3/model.safetensors',\n",
    "    \n",
    "# ]\n",
    "# checkpoints = [\n",
    "#     '/home/jovyan/gkuzmin/rmt_it/data/pretrained_models/RMT-Llama-3.2-1B-Instruct-8x1024-mem16-lora-babilong-qa1-5_ct-v3.1/model.safetensors'\n",
    "# ]\n",
    "\n",
    "checkpoints = [\n",
    "    # \"/home/jovyan/rmt/runs/test/babilong_multitask/meta-llama/Llama-3.2-1B-Instruct/lr_3e-04_d64_cosine_adamw_wd1e-03_1x1024_mem16_bs64_bptt--1_from_cpt_0-1_lora_ct-v3/run_1/checkpoint-6500/pytorch_model.bin\",\n",
    "    # \"/home/jovyan/rmt/runs/test/babilong_multitask/meta-llama/Llama-3.2-1B-Instruct/lr_3e-04_d64_cosine_adamw_wd1e-03_2x1024_mem16_bs64_bptt--1_from_cpt_1-2_lora_ct-v3/run_1/checkpoint-28000/pytorch_model.bin\",\n",
    "    # \"/home/jovyan/rmt/runs/test/babilong_multitask/meta-llama/Llama-3.2-1B-Instruct/lr_3e-04_d64_cosine_adamw_wd1e-03_4x1024_mem16_bs64_bptt--1_from_cpt_2-4_lora_ct-v3/run_1/checkpoint-24500/pytorch_model.bin\",\n",
    "    \"/home/jovyan/rmt/runs/test/babilong_multitask/meta-llama/Llama-3.2-1B-Instruct/lr_3e-04_d64_cosine_adamw_wd1e-03_8x1024_mem16_bs64_bptt--1_from_cpt_4-8_lora_ct-v3/run_1/checkpoint-30000/pytorch_model.bin\",\n",
    "    # \"/home/jovyan/rmt/runs/test/babilong_multitask/meta-llama/Llama-3.2-1B-Instruct/lr_3e-04_d64_cosine_adamw_wd1e-03_16x1024_mem16_bs64_bptt--1_from_cpt_8-16_lora_ct-v3/run_1/checkpoint-6000/pytorch_model.bin\",\n",
    "]\n",
    "\n",
    "eval_model_template = 'rmt-llama3.2-1b-{}-ct-v3-align_right'\n",
    "\n",
    "dataset_name = \"RMT-team/babilong\"\n",
    "results_folder = \"/home/jovyan/rmt/babilong/babilong_evals/test/\"\n",
    "# results_folder = \"/home/jovyan/gkuzmin/rmt_it/rmt_ift/babilong_evals/DEBUG_eval_versions/orig_eval_8s_best_bf16_no_autocast_ainst_right_our_loading/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_kwargs = {\n",
    "    'max_new_tokens': 30,\n",
    "    'max_length': None,\n",
    "    'num_beams': 1,\n",
    "    'do_sample': False,\n",
    "    'temperature': None,\n",
    "    'top_p': None,\n",
    "    'top_k': None,\n",
    "    'pad_token_id': tokenizer.pad_token_id\n",
    "}\n",
    "\n",
    "if generate_kwargs['pad_token_id'] is None:\n",
    "    generate_kwargs['pad_token_id'] = tokenizer.eos_token_id\n",
    "\n",
    "# print(f'prompt template:\\n{DEFAULT_TEMPLATE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "# model_cpt = '/home/jovyan/gkuzmin/rmt_it/data/pretrained_models/RMT-Llama-3.2-1B-Instruct-8x1024-mem16-lora-babilong-qa1-5_ct-v3.1/model.safetensors'\n",
    "model_cpt = checkpoints[0]\n",
    "use_peft = 1\n",
    "segment_size = 1056\n",
    "max_n_segments = 32\n",
    "layers_attr = \"model.layers\"\n",
    "add_question_prompt = \"Answer with a single word.\"\n",
    "segment_alignment = \"right\"\n",
    "lora_r = 8\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.1\n",
    "mem_size = 16\n",
    "d_mem = 64\n",
    "wrap_pos = False\n",
    "no_correction = False\n",
    "attend_to_previous_input = 0\n",
    "api_url = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"/home/jovyan/kuratov/models/Llama-3.2-1B-Instruct/\"\n",
    "# model_name = ''\n",
    "# model_name = \"unsloth/Llama-3.2-1B-Instruct\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.bfloat16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "if model_name == \"meta-llama/Llama-3.2-1B\":\n",
    "    # hotfix - load 8b instruct tokenizer and reuse chat template from instruct tokenizer\n",
    "    it_tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer.chat_template = it_tokenizer.chat_template\n",
    "if not api_url:\n",
    "    if len(model_cpt) != 0:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True,\n",
    "                                                     device_map='cpu', torch_dtype=dtype,\n",
    "                                                     attn_implementation='flash_attention_2')\n",
    "        if use_peft:\n",
    "            peft_config = LoraConfig(\n",
    "                task_type=TaskType.CAUSAL_LM, \n",
    "                inference_mode=False, \n",
    "                r=lora_r, \n",
    "                lora_alpha=lora_alpha, \n",
    "                lora_dropout=lora_dropout,\n",
    "                )\n",
    "            model = get_peft_model(model, peft_config)\n",
    "        device = \"cpu\"\n",
    "        if d_mem is not None:\n",
    "            mem_cell_args = dict(\n",
    "                base_model=model,\n",
    "                num_mem_tokens=mem_size,\n",
    "            )\n",
    "            # additional parameters for ARMT model\n",
    "            mem_cell_args['d_mem'] = d_mem\n",
    "            mem_cell_args['wrap_pos'] = wrap_pos\n",
    "            mem_cell_args['correction'] = not(no_correction)\n",
    "            mem_cell_args['use_lora'] = use_peft\n",
    "            if layers_attr is not None:\n",
    "                mem_cell_args['layers_attr'] = layers_attr\n",
    "            # if attend_to_previous_input is not None:\n",
    "            #     mem_cell_args['attend_to_previous_input'] = bool(attend_to_previous_input)\n",
    "            cell = AssociativeMemoryCell(**mem_cell_args)\n",
    "            model = AssociativeRecurrentWrapper(cell,\n",
    "                                                segment_size=segment_size-2*mem_size,\n",
    "                                                max_n_segments=max_n_segments,\n",
    "                                                segment_alignment=segment_alignment,\n",
    "                                                attend_to_previous_input=attend_to_previous_input,\n",
    "            ).to(device)\n",
    "        else:\n",
    "            cell = MemoryCell(model, num_mem_tokens=mem_size)\n",
    "            model = RecurrentWrapper(cell,\n",
    "                                     segment_size=segment_size-2*mem_size,\n",
    "                                     max_n_segments=max_n_segments,\n",
    "            ).to(device)\n",
    "        #model = model.to(dtype)\n",
    "        try:\n",
    "            cpt = torch.load(model_cpt, map_location=device)    \n",
    "            model.load_state_dict(cpt, strict=True)\n",
    "        except:\n",
    "            # if the model saved in safetensors\n",
    "            from safetensors.torch import load_model\n",
    "            load_model(model, model_cpt, device=device)\n",
    "            #model.load_state_dict(model_cpt, map_location=device)\n",
    "        \n",
    "        device = 'cuda:0'\n",
    "        model.to(device)\n",
    "        model.to(dtype)\n",
    "        model.name_or_path = \"custom_rmt\" # workaround\n",
    "        model.device = device\n",
    "    # load the model locally if llamacpp API is not used\n",
    "    else:\n",
    "        try:\n",
    "            print('trying to load model with flash attention 2...')\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True,\n",
    "                                                         device_map='auto', torch_dtype=dtype,\n",
    "                                                         attn_implementation='flash_attention_2')\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            print('trying to load model without flash attention 2...')\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True,\n",
    "                                                         device_map='auto', torch_dtype=dtype)\n",
    "\n",
    "    model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'segment_size': 1024,\n",
       " 'max_n_segments': 32,\n",
       " 'segment_alignment': 'right',\n",
       " 'attend_to_previous_input': 0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.rmt_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128009"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory tensor([ 0.0070, -0.0124,  0.0410, -0.0322,  0.0082,  0.0262,  0.0776,  0.0442,\n",
      "        -0.0332,  0.0835], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<SliceBackward0>)\n",
      "model.base_model.model.model.embed_tokens.weight tensor([ 0.0031,  0.0178,  0.0210, -0.0101,  0.0070,  0.0068,  0.0139,  0.0052,\n",
      "         0.0094,  0.0062], device='cuda:0', dtype=torch.bfloat16)\n",
      "model.base_model.model.model.layers.0.W_mq.weight tensor([-0.0320, -0.0195,  0.0037, -0.0053,  0.0206,  0.0063, -0.0074,  0.0386,\n",
      "        -0.0120,  0.0060], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<SliceBackward0>)\n",
      "model.base_model.model.model.layers.0.W_mk.weight tensor([ 0.0117,  0.0054,  0.0014, -0.0046,  0.0020, -0.0098, -0.0021, -0.0262,\n",
      "        -0.0089,  0.0078], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<SliceBackward0>)\n",
      "model.base_model.model.model.layers.0.W_mv.weight tensor([ 0.0135, -0.0090, -0.0017,  0.0042, -0.0049, -0.0041,  0.0140, -0.0074,\n",
      "         0.0035, -0.0069], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<SliceBackward0>)\n",
      "model.base_model.model.model.layers.0.W_mb.weight tensor([ 0.0325, -0.0270, -0.0272,  0.0271,  0.0742,  0.0216,  0.0598, -0.0245,\n",
      "        -0.0217,  0.0098], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[118], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n, p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mmemory_cell\u001b[38;5;241m.\u001b[39mnamed_parameters():\n",
      "\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(n, \u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "for n, p in model.memory_cell.named_parameters():\n",
    "    print(n, p[0, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(cpt, strict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory tensor([ 0.0070, -0.0124,  0.0410, -0.0322,  0.0082,  0.0262,  0.0776,  0.0442,\n",
      "        -0.0332,  0.0835], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<SliceBackward0>)\n",
      "model.base_model.model.model.embed_tokens.weight tensor([ 0.0031,  0.0178,  0.0210, -0.0101,  0.0070,  0.0068,  0.0139,  0.0052,\n",
      "         0.0094,  0.0062], device='cuda:0', dtype=torch.bfloat16)\n",
      "model.base_model.model.model.layers.0.W_mq.weight tensor([-0.0320, -0.0195,  0.0037, -0.0053,  0.0206,  0.0063, -0.0074,  0.0386,\n",
      "        -0.0120,  0.0060], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<SliceBackward0>)\n",
      "model.base_model.model.model.layers.0.W_mk.weight tensor([ 0.0117,  0.0054,  0.0014, -0.0046,  0.0020, -0.0098, -0.0021, -0.0262,\n",
      "        -0.0089,  0.0078], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<SliceBackward0>)\n",
      "model.base_model.model.model.layers.0.W_mv.weight tensor([ 0.0135, -0.0090, -0.0017,  0.0042, -0.0049, -0.0041,  0.0140, -0.0074,\n",
      "         0.0035, -0.0069], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<SliceBackward0>)\n",
      "model.base_model.model.model.layers.0.W_mb.weight tensor([ 0.0325, -0.0270, -0.0272,  0.0271,  0.0742,  0.0216,  0.0598, -0.0245,\n",
      "        -0.0217,  0.0098], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n, p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mmemory_cell\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(n, \u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "for n, p in model.memory_cell.named_parameters():\n",
    "    print(n, p[0, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0031,  0.0178,  0.0210, -0.0101,  0.0070,  0.0068,  0.0139,  0.0052,\n",
       "          0.0094,  0.0062,  0.0052,  0.0008, -0.0009,  0.0021, -0.0247,  0.0254,\n",
       "         -0.0078,  0.0267,  0.0096, -0.0137]], device='cuda:0',\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.memory_cell.model.base_model.model.model.embed_tokens.weight[0:1, 0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
       "        device='cuda:0')}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = torch.load(\"tmp\")\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_new_tokens': 30,\n",
       " 'max_length': None,\n",
       " 'num_beams': 1,\n",
       " 'do_sample': False,\n",
       " 'temperature': None,\n",
       " 'top_p': None,\n",
       " 'top_k': None,\n",
       " 'pad_token_id': 128004}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    attn_mask = torch.ones_like(model_inputs['input_ids'].bool().to(device))\n",
    "    # output = model.generate(**model_inputs, **generate_kwargs, attention_mask=attn_mask)\n",
    "    output = model.generate(**model_inputs, **generate_kwargs, attention_mask=attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    65,  78932, 128009]], device='cuda:0')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n, p in model.memory_cell.model.base_model.named_parameters():\n",
    "#     print(n, p[:10])a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n, p in model.named_parameters():\n",
    "#     print(n, p[0, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.memory_cell.model.base_model.model.model.layers[0].layer.self_attn.q_proj.lora_A.default.weight[0:1, 0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(cpt, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.memory_cell.model.disable_adapter_layers()\n",
    "# model.memory_cell.model.enable_adapter_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating  rmt-llama3.2-1b-/lr-ct-v3-align_right\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8edc9d5b1c4347ab991359f9ec4e43d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tasks:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2814cc99bee4a0eb22a70c187ac5ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "lengths:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since RMT-team/babilong couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration '16k' at /home/jovyan/.cache/huggingface/datasets/RMT-team___babilong/16k/0.0.0/ee0d588794c7ac098062ee0d247c733d62e94fe2 (last modified on Thu Aug  1 18:01:42 2024).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d81bed3eebca4564b592d25267a464ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "task: qa1 length: 16k:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 91\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# write results to csv file\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m# df.to_csv(outfile)\u001b[39;00m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m7\u001b[39m:\n\u001b[0;32m---> 91\u001b[0m         \u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "tasks = ['qa1']\n",
    "tasks = ['qa1', 'qa2', 'qa3', 'qa4', 'qa5']\n",
    "split_names = ['0k', '1k', '2k', '4k', '8k', '16k']\n",
    "split_names = ['16k']\n",
    "\n",
    "use_chat_template = True\n",
    "use_instruction = False\n",
    "use_post_prompt = False\n",
    "use_examples = False\n",
    "#model = model.to(torch.bfloat16)\n",
    "for cpt_path in checkpoints:\n",
    "    eval_model_name = eval_model_template.format(cpt_path.split('Llama-3.2-1B-Instruct')[1].split('_')[0])\n",
    "    print('Evaluating ', eval_model_name)\n",
    "    \"\"\"\n",
    "    device = 'cuda:0'\n",
    "    if \"safetensors\" in cpt_path:\n",
    "        load_model(model, cpt_path, device=device)\n",
    "    else:\n",
    "        with open(cpt_path, 'rb') as cpt:\n",
    "            weights = torch.load(cpt)\n",
    "\n",
    "        model.load_state_dict(weights)\n",
    "    model.cuda()\n",
    "    model = model.to(torch.bfloat16)\n",
    "    model.eval()\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    for task in tqdm(tasks, desc='tasks'):\n",
    "        # configure the prompt\n",
    "        prompt_cfg = {\n",
    "            'instruction': DEFAULT_PROMPTS[task]['instruction'] if use_instruction else '',\n",
    "            'examples': DEFAULT_PROMPTS[task]['examples'] if use_examples else '',\n",
    "            'post_prompt': DEFAULT_PROMPTS[task]['post_prompt'] if use_post_prompt else '',\n",
    "            'template': DEFAULT_TEMPLATE,\n",
    "            'chat_template': use_chat_template,\n",
    "        }\n",
    "        prompt_name = [f'{k}_yes' if prompt_cfg[k] else f'{k}_no' for k in prompt_cfg if k != 'template']\n",
    "        prompt_name = '_'.join(prompt_name)\n",
    "\n",
    "        for split_name in tqdm(split_names, desc='lengths'):\n",
    "            # load dataset\n",
    "            data = datasets.load_dataset(dataset_name, split_name)\n",
    "            task_data = data[task]\n",
    "\n",
    "            # Prepare files with predictions, prompt, and generation configurations\n",
    "            outfile = Path(f'{results_folder}/{eval_model_name}/{task}_{split_name}_{prompt_name}.csv')\n",
    "            outfile.parent.mkdir(parents=True, exist_ok=True)\n",
    "            cfg_file = f'{results_folder}/{eval_model_name}/{task}_{split_name}_{prompt_name}.json'\n",
    "            json.dump({'prompt': prompt_cfg, 'generate_kwargs': generate_kwargs}, open(cfg_file, 'w'), indent=4)\n",
    "\n",
    "            df = pd.DataFrame({'target': [], 'output': [], 'question': []})\n",
    "\n",
    "            for sample in tqdm(task_data, desc=f'task: {task} length: {split_name}'):\n",
    "                target = sample['target']\n",
    "                context = sample['input']\n",
    "                question = sample['question']\n",
    "\n",
    "                # format input text\n",
    "                input_text = get_formatted_input(context, question + 'Answer with a single word.', prompt_cfg['examples'],\n",
    "                                                    prompt_cfg['instruction'], prompt_cfg['post_prompt'],\n",
    "                                                    template=prompt_cfg['template'])\n",
    "\n",
    "                if use_chat_template:\n",
    "                    input_text = [{'role': 'user', 'content': input_text}]\n",
    "                    model_inputs = tokenizer.apply_chat_template(input_text, add_generation_prompt=True,\n",
    "                                                                    return_tensors='pt').to(device)\n",
    "                    model_inputs = {'input_ids': model_inputs}\n",
    "                else:\n",
    "                    model_inputs = tokenizer(input_text, return_tensors='pt',\n",
    "                                                add_special_tokens=True).to(device)\n",
    "\n",
    "                sample_length = model_inputs['input_ids'].shape[1]\n",
    "                #with torch.cuda.amp.autocast():\n",
    "                with torch.no_grad():\n",
    "                    attn_mask = torch.ones_like(model_inputs['input_ids'].bool().to(device))\n",
    "                    output = model.generate(**model_inputs, **generate_kwargs, attention_mask=attn_mask)\n",
    "                    # we need to reset memory states between samples for activation-beacon models\n",
    "                    # if 'activation-beacon' in model.name_or_path and hasattr(model, 'memory'):\n",
    "                    #     model.memory.reset()\n",
    "\n",
    "                output = output[0]#[sample_length:]\n",
    "                output = tokenizer.decode(output, skip_special_tokens=True).strip()\n",
    "\n",
    "                df.loc[len(df)] = [target, output, question]\n",
    "            # write results to csv file\n",
    "                # df.to_csv(outfile)\n",
    "                if df.shape[0] > 7:\n",
    "                    1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>output</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bathroom</td>\n",
       "      <td>b b b b b b b b b b b b b b b b b b b b b b b ...</td>\n",
       "      <td>Where is Mary?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kitchen</td>\n",
       "      <td>hallhallhallhallhallhallhallhallhallhallhallha...</td>\n",
       "      <td>Where is Sandra?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kitchen</td>\n",
       "      <td>bedbedbedbedbedbedbedbedbedbedbedbedbedbedbedb...</td>\n",
       "      <td>Where is Mary?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kitchen</td>\n",
       "      <td>gggggggggggggggggggggggggggggg</td>\n",
       "      <td>Where is John?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bedroom</td>\n",
       "      <td>Kbedbed Kbedbed Kbedbed Kbedbed Kbedbed Kbedbe...</td>\n",
       "      <td>Where is Sandra?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>office</td>\n",
       "      <td>gggggggggggggggggggggggggggggg</td>\n",
       "      <td>Where is John?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>garden</td>\n",
       "      <td>bbbbbbbbbbbbbbbbbbbbbbbbbbbbbb</td>\n",
       "      <td>Where is Mary?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bathroom</td>\n",
       "      <td>bedbedbedbedbedbedbedbedbedbedbedbedbedbedbedb...</td>\n",
       "      <td>Where is Sandra?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target                                             output  \\\n",
       "0  bathroom  b b b b b b b b b b b b b b b b b b b b b b b ...   \n",
       "1   kitchen  hallhallhallhallhallhallhallhallhallhallhallha...   \n",
       "2   kitchen  bedbedbedbedbedbedbedbedbedbedbedbedbedbedbedb...   \n",
       "3   kitchen                     gggggggggggggggggggggggggggggg   \n",
       "4   bedroom  Kbedbed Kbedbed Kbedbed Kbedbed Kbedbed Kbedbe...   \n",
       "5    office                     gggggggggggggggggggggggggggggg   \n",
       "6    garden                     bbbbbbbbbbbbbbbbbbbbbbbbbbbbbb   \n",
       "7  bathroom  bedbedbedbedbedbedbedbedbedbedbedbedbedbedbedb...   \n",
       "\n",
       "            question  \n",
       "0    Where is Mary?   \n",
       "1  Where is Sandra?   \n",
       "2    Where is Mary?   \n",
       "3    Where is John?   \n",
       "4  Where is Sandra?   \n",
       "5    Where is John?   \n",
       "6    Where is Mary?   \n",
       "7  Where is Sandra?   "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>output</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bathroom</td>\n",
       "      <td>bathroom</td>\n",
       "      <td>Where is Mary?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kitchen</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>Where is Sandra?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kitchen</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>Where is Mary?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kitchen</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>Where is John?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bedroom</td>\n",
       "      <td>bedroom</td>\n",
       "      <td>Where is Sandra?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>office</td>\n",
       "      <td>office</td>\n",
       "      <td>Where is John?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>garden</td>\n",
       "      <td>garden</td>\n",
       "      <td>Where is Mary?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bathroom</td>\n",
       "      <td>bathroom</td>\n",
       "      <td>Where is Sandra?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target    output           question\n",
       "0  bathroom  bathroom    Where is Mary? \n",
       "1   kitchen   kitchen  Where is Sandra? \n",
       "2   kitchen   kitchen    Where is Mary? \n",
       "3   kitchen   kitchen    Where is John? \n",
       "4   bedroom   bedroom  Where is Sandra? \n",
       "5    office    office    Where is John? \n",
       "6    garden    garden    Where is Mary? \n",
       "7  bathroom  bathroom  Where is Sandra? "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>output</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bathroom</td>\n",
       "      <td>bathroom</td>\n",
       "      <td>Where is Mary?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kitchen</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>Where is Sandra?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kitchen</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>Where is Mary?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kitchen</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>Where is John?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bedroom</td>\n",
       "      <td>bedroom</td>\n",
       "      <td>Where is Sandra?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>office</td>\n",
       "      <td>office</td>\n",
       "      <td>Where is John?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>garden</td>\n",
       "      <td>garden</td>\n",
       "      <td>Where is Mary?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bathroom</td>\n",
       "      <td>bathroom</td>\n",
       "      <td>Where is Sandra?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target    output           question\n",
       "0  bathroom  bathroom    Where is Mary? \n",
       "1   kitchen   kitchen  Where is Sandra? \n",
       "2   kitchen   kitchen    Where is Mary? \n",
       "3   kitchen   kitchen    Where is John? \n",
       "4   bedroom   bedroom  Where is Sandra? \n",
       "5    office    office    Where is John? \n",
       "6    garden    garden    Where is Mary? \n",
       "7  bathroom  bathroom  Where is Sandra? "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bathroom'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bathroom'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = model.segment(input_ids=model_inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 594]),\n",
       " torch.Size([1, 1024]),\n",
       " torch.Size([1, 1024]),\n",
       " torch.Size([1, 1024])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s['input_ids'].shape for s in segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
